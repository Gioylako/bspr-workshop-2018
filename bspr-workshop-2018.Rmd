--- 
title: "Data Science Workshop"
subtitle: "British Society for Proteomic Research Meeting 2018"
author: "Alistair Bailey"
date: "`r format(Sys.Date(),format='%B %d %Y')`"
output: 
  bookdown::gitbook: default
  bookdown::pdf_book:
    includes:
      in_header: preamble.tex
    keep_tex: yes
    citation_package: natbib
    latex_engine: xelatex
documentclass: book
fontsize: 12pt
linestretch: 1.5
toc-depth: 1
secnumdepth: 1
lof: FALSE
lot: FALSE
site: bookdown::bookdown_site
bibliography: [packages.bib, book.bib]
biblio-style: apalike
link-citations: yes
#geometry: "left=4cm, right=3cm, top=2.5cm, bottom=2.5cm"
description: "Lessons for the BSPR 2018 Data Science Workshop"
---

```{r include=FALSE}
library(tidyverse)
library(janitor)
library(cowplot)
library(gplots)
library(pheatmap)
library(gridExtra)
library(VennDiagram)
library(ggseqlogo)
# automatically create a bib database for R packages
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown', 
                   "tidyverse","gplots","pheatmap", 'VennDiagram','gridExtra',
                   'ggseqlogo'), 
                 'packages.bib')

options(knitr.graphics.auto_pdf = TRUE)
```

# Overview {-}

This book covers:

1. An introduction to R and RStudio
2. An introduction to tidyverse and base R
3. Importing and transforming proteomics data
4. Visualisation of proteomics analysis

The analysis is of an example data set of observations for 7702 proteins
from cells in three control experiments and three treatment experiments.
The observations are signal intensity measurements from the mass spectrometer.
These intensities relate the concentration of protein observed in each experiment 
and under each condition. The analysis transforms the data to examine the effect of
treatment on the cellular proteome and visualise the output using a volcano
plot , a heatmap, a Venn diagram and peptide sequence logos. 
<a href="https://raw.githubusercontent.com/ab604/ab604.github.io/master/docs/070718-proteomics-example-data.csv" download>Click here to download the csv file.</a>

## Requirements {-}

An up to date version of R [@R-base] and RStudio 
[@rstudioteam2018]. 

If you are new to R, then the first thing
to know is that R is a programming language and RStudio is a
program for working with R called an integrated development 
environment (IDE). You can use R without RStudio, but not the other way around. Further details in Chapter 
\@ref(r-rstudio).

[Download R here](https://cran.r-project.org/) and 
[Download RStudio Desktop here](https://www.rstudio.com/products/rstudio/download/).

These materials were generated using R version 3.5.0.

Once you've installed R and RStudio, you'll also need
a few R packages. Packages are collections of [functions](#function-anatomy). 

Open RStudio and put the code below into the `Console` 
window and press `Enter` to install these three packages.

```{r packages, eval=FALSE}
install.packages(c("plyr","tidyverse","gplots","pheatmap",
                   "gridExtra","VennDiagram","ggseqlogo"))
```




<!--chapter:end:index.Rmd-->

# Introduction {#intro}

There are many resources for learning R on the web. Much of Chapters \@ref(intro), 
\@ref(tidyverse), \@ref(import) and \@ref(dplyr) derive from 
a [Data Carpentry lesson](http://www.datacarpentry.org/lessons/) using 
ecological data that I have previously [reworked](https://southampton-rsg.github.io/2017-08-01-southampton-dc/novice/R-ecology-lesson/index.html), 
which in turn takes a lot from [Hadley Wickham's R for Data Science](http://r4ds.had.co.nz/) aka **R4DS**. 
Follow the links to access those materials.

Chapter \@ref(transform) deals with some statistical transformations and 
visualisation methods in the context of proteomics data. 

Whilst finally in 
Chapter \@ref(going-further) there is some advice about how to build upon the 
materials covered here.

In terms of philosophy:

1. The primary motivation for using tools such as R is to get more done, in
less time and with less pain.

2. And the overall aim is to *understand and communicate* findings from our
data.

(ref:pipeline) Data project workflow.

```{r pipeline, fig.cap='(ref:pipeline)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/data_project_pipeline.png")
```

As shown in Figure \@ref(fig:pipeline) of typical data analysis workflow,
to acheive this aim we need to learn tools that enable us to perform the 
fundamental tasks of tasks of importing, tidying and 
often transforming the data. Transformation means for example, selecting a 
subset of the data to work with, or calculating the mean of a set of observations.
We'll cover that in Chapter \@ref(transform).

But first...

## What are R and RStudio? {#r-rstudio}

***"There are only two kinds of languages: the ones people complain about and the ones nobody uses"***

*Bjarne Stroustrup*

**R** is a programming language that follows the philosophy laid down by it's
predecessor S. The philosophy being that users begin in an interactive
environment where they don't consciously think of themselves as programming. 
It was created in 1993, and documented in [@ihaka1996].

Reasons R has become popular include that it is both open source and cross platform,
and that it has broad functionality, from the analysis of data and creating 
powerful graphical visualisations and web apps.

Like all languages though it has limitations, for example the syntax is initially
confusing. 

Take for example the word `environment`...

### Environments

An environment is where we bring our data to work with it. Here we work in a R 
envrionment, using the R language as a set of tools.
**RStudio** is an integrated development environment, or IDE for R programming. 
It is regularly updated, and upgrading enables access to the latest features. 

The latest version can be downloaded here: http://www.rstudio.com/download

## Why learn R, or any language ?

We can write R code without saving it, but it's generally more useful to write 
and save our code as a script. Working with scripts makes the steps you used in 
your analysis clear, and the code you write can be inspected by someone else who 
can give  you feedback and spot mistakes.

Learning R (or any programming language) and working with scripts forces you to 
have deeper understanding of what you are doing, facilitates your learning 
and comprehension of the methods you use:

+ Writing and publishing code is important for reproducible resarch
+ R has many thousands of packages covering many disciplines.
+ R can work with many types of data.
+ They is a large R community for development and support.
+ Using R gives you control over your figures and reports.

## Finding your way around RStudio

Let's begin by learning about [RStudio](https://www.rstudio.com/), the
Integrated Development Environment (IDE).

We will use R Studio IDE to write code, navigate the files found on our computer,
inspect the variables we are going to create, and visualize the plots we will
generate. R Studio can also be used for other things (e.g., version control,
developing packages, writing Shiny apps) that we don't have time to cover during
this workshop.

R Studio is divided into "Panes", see Figure \@ref(fig:rstudio). 

When you first open it, there are three panes,the console where you type 
commands, your environment/history (top-right), and your
files/plots/packages/help/viewer (bottom-right).

The enivronment shows all the R objects you have created or are using, such
as data you have imported.

The output pane can be used to view any plots you have created.

Not opened at first start up is the fourth default pane: the script editor pane, 
but this will open as soon as we create/edit a R script (or many other document types). 
*The script editor is where will be typing much of the time.*


(ref:rstudio) The Rstudio Integrated Development Environment (IDE).

```{r rstudio,fig.cap='(ref:rstudio)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/rstudio_ide_image.png")
```

The placement of these panes and their content can be customized (see menu, 
R Studio -> Tools -> Global Options -> Pane Layout). One of the advantages of 
using R Studio is that all the information you need to write code is available 
ina single window. Additionally, with many shortcuts, auto-completion, and 
highlighting for the major file types you use while developing in R, R Studio 
will make typing easier and less error-prone.

Time for a philosphical diversion...

### What is real?

At the start, we might consider our environment "real" - that is to say the objects
we've created/loaded and are using are "real". But it's much better in the long run
to consider our scripts as "real" - our scripts are where we write down the code
that creates our objects that we'll be using in our environment.

**As a script is a document, it is reproducible**

Or to put it another way: we can easily recreate an environment from our scripts, 
but not so easily create a script from an enivronment.

To support this notion of thinking in terms of our scripts as real, we recommend
turning off the preservation of workspaces between sessions by setting the 
Tools > Global Options menu in R studio as shown in Figure \@ref(fig:workspace):

(ref:turn-off) Don't save your workspace, save your script instead.


```{r workspace, fig.cap='(ref:turn-off)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/rdata_turn_off.png")
```

## Where am I?

R studio tells you where you are in terms of directory address like so:

(ref:working-dir) Your working directory

```{r working-directory,fig.cap='(ref:working-dir)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/rstudio_working_directory.png")
```

If you are unfamiliar with how computers structure folders and files, then 
consider a tree with a root from which the trunk extends and branches divide.
In the image above, the ~ symbol represents a contraction of the path from the
root to the 'home' directory (in Windows this is 'Documents') and then the 
forward slashes are the branches. (Note: Windows uses backslashes, Unix type 
systems and R use forwardslashes).

It is good practice to keep a set of related data, analyses, and text
self-contained in a single folder, called the **working directory**. All of the
scripts within this folder can then use *relative paths* to files that indicate
where inside the project a file is located (as opposed to absolute paths, which
point to where a file is on a specific computer). Working this way makes it
a lot easier to move your project around on your computer and share it with
others without worrying about whether or not the underlying scripts will still
work.

(ref:dir-structure) A typical directory structure

```{r dir-structure,fig.cap='(ref:dir-structure)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/R-ecology-work_dir_structure.png")
```

## R projects

RStudio also has a facility to keep all files associated with a particular 
analysis together called a project.

Creating a project creates a working directory for you and also remembers
its location (allowing you to quickly navigate to it) and optionally preserves
custom settings and open files to make it easier to resume work after a
break. 

(ref:r-project) Creating a R project

```{r r-projects,fig.cap='(ref:r-project)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/rstudio_create_project.PNG")
```

Below, we will go through the steps for creating an "R Project":

* Start R Studio (presentation of R Studio -below- should happen here)
* Under the `File` menu, click on `New project`, choose `New directory`, then
  `Empty project`
* Enter a name for this new folder (or "directory", in computer science), and
  choose a convenient location for it. This will be your **working directory**
  for the rest of the day (e.g., `~/bspr-workshop`)
* Click on "Create project"
* Under the `Files` tab on the right of the screen, click on `New Folder` and
  create a folder named `data` within your newly created working directory. (e.g., `~/bspr-workshopdata`)
* Create a new R script (File > New File > R script) and save it in your working
  directory (e.g. `bspr-workshop-script.R`)
  
## Naming things {#names}
  
[Jenny Bryan](https://ropensci.org/blog/2017/12/08/rprofile-jenny-bryan/)
has three principles for [naming things](http://www2.stat.duke.edu/~rcs46/lectures_2015/01-markdown-git/slides/naming-slides/naming-slides.pdf) that are well worth remembering.

When you names something, a file or an object, ideally it should be:
  
1. Machine readable (no whitespace, punctuation, upper AND lowercase...)
2. Human readable  (makes sense in 6 months or 2 years time)
3. Plays well with default ordering (numerical or date order)

## Seeking help

If you need help with a specific R function, let's say `barplot()`, you can type:

```{r barplot, eval=FALSE}
?barplot
```


If you can't find what you are looking for, you can use the
[rdocumention.org](http://www.rdocumentation.org) website that searches through
the help files across all packages available.

A Google or internet search "R \<task\>" will often either send you to the appropriate package documentation or a helpful forum question that someone else already asked,
such as [Stack Overflow](http://stackoverflow.com/questions/tagged/r) or
the [RStudio Community](https://community.rstudio.com/).

### Asking for help

As well as knowing [where to ask](https://www.tidyverse.org/help/#where-to-ask), the key to get help 
from someone is for them to grasp your problem rapidly. You
should make it as easy as possible to pinpoint where the issue might be.

Try to use the correct words to describe your problem. For instance, a 
package is not the same thing as a library. Most people will understand 
what you meant, but others have really strong feelings about the difference
in meaning. The key point is that it can make things confusing for people 
trying to help you. Be as precise as possible when describing your problem.

If possible, try to reduce what doesn't work to a simple *reproducible
example* otherwise known as a *reprex*. 

For more information on how to write a reproducible example see 
[this article](https://www.tidyverse.org/help/#reprex).

<!--chapter:end:01-intro.Rmd-->

# Getting started in R and the tidyverse {#tidyverse}

Functions are a way to automate common tasks and R comes with a set of functions 
called the `base` package. We will be using some `base` functions in 
Chapter \@ref(transform), but to introduce the concept of using [functions](#function-anatomy)
we'll begin with the `tidyverse`.

## The tidyverse and tidy data

The [tidyverse](https://www.tidyverse.org/) [@R-tidyverse] is *"an opinionated 
collection of R packages designed for data science"* . 

Tidyverse packages contain functions that *"share an underlying design 
philosophy, grammar, and data structures."* It's this philiosophy that makes 
tidyverse functions and packages relatively easy to learn and use.

Tidy data follows three principals for tabular data as proposed in the Tidy Data 
paper http://www.jstatsoft.org/v59/i10/paper :

1. Every variable has its own column.
2. Every observation has its own row.
3. Each value has its own cell.

If our table was proteomics data then, we might have a set of variables such 
as the peptide sequence, mass or length observed for a number of peptides.
Therefore each peptide would have a row with columns for peptide sequence, mass 
and length with the value for each variable in separate cells, as
seen in Figure \@ref(fig:tidy-prot).

(ref:tidy-prot) An example of tidy proteomics data

```{r tidy-prot, fig.cap='(ref:tidy-prot)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/tidy_prot_data.PNG")
```

Often much of the work in any data analysis is getting our data into a tidy form.

We can't do everything in the tidyverse, and everything we can do in the 
tidyverse can be done in what is called base R or other packages, but the 
motivation behind the tidyverse is to ease the pain of data manipulation.

With this in mind, the two tasks we are most likely to want to do 
in data science are:

1. Visualise our data
2. Automate our processes.

Taking our cue from [R4DS](http://r4ds.had.co.nz/) let's try an example.

## Data visualisation

The `ggplot2` package implements the *grammer of graphics*, for describing
and building graphs.

The motivation here is twofold: 

1. To begin to grasp the grammar of graphics approach to creating plots. This will
be our first example of automating a task using a function.
2. To demonstrate how plotting is often the most useful thing we can do when 
trying to understand our data.

We'll use the `mpg` dataset that comes with the tidyverse to examine
the question *do cars with big engines use more fuel than cars with small engines?*

Try `?mpg` to learn more about the data.

1. Engine size in litres is in the `displ` column.
2. Fuel efficiency on the highway in miles per gallon is given in the `hwy` column.

To create a plot of engine size `displ` (x-axis) against fuel efficiency `hwy` (y-axis)  we do the following:

1. Use the `ggplot()` function to create an empty graph.
2. Provide ggplot with a first input or **argument** of the data (here `mpg`). 
3. Then we follow the ggplot function with a `+` sign to indicate
we are going to add more code, followed by a `geom_point()` function to add a 
layer of points mapping some aesthetics for the x and y axes.
4. Mapping is always paired to aesthetics `aes()`. An aesthetic is a visual 
property of the objects in your plot, such a point size, shape or point colour.

Therefore to plot engine size (x-axis) against fuel efficiency (y-axis) we 
use the following code:

```{r mpg-plot-1,mpg_point_plot}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
```

This plot shows a negative relationship between engine size and fuel 
efficiency.

Now try extending this code to include to add a `colour` aesthetic to the
the `aes()` function, let `colour = class`, `class` being the veichle type.
This should create a plot with as before but with the points coloured
according to the viechle type to expand our understanding.

```{r, mpg-plot-2}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, colour = class))
```

Now we can see that as we might expect, bigger cars such as SUVs tend to have 
bigger engines and are also less fuel efficient, but some smaller cars such as 
2-seaters also have big engines and greater fuel efficiency. Hence we have a 
more nuanced view with this additional aesthetic.

Check out the ggplot2 documentation for all the aesthetic possibilities (and 
Google for examples): http://ggplot2.tidyverse.org/reference/

So now we have re-usable code snippet for generating plots in R:

```{r, purl=FALSE,eval=FALSE}
ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
```

Concretely, in our first example `<DATA>` was `mpg`, the `<GEOM_FUNCTION>` 
was `geom_point()` and the arguments we supplies to map our aesthetics 
`<MAPPINGS>` were `x = displ, y = hwy`.

As we can use this code for any tidy data set, hopefully you are beginning to 
see how a small amount of code can do a lot.

### Visualisations 

Claus Wilke has written a very nice guide to visualising data using R called
[Fundamentals of Data Visualization](http://serialmentor.com/dataviz/index.html).

## Workflow basics

Let's run through the basics of working in R to conclude this chapter.

### Assigning objects

Objects are just a way to store data inside the R environment. We create objects using the assignment operator `<-`:

```{r mass-kg}
mass_kg <- 55
```

Read this as *"mass_kg gets value 55"* in your head.

Using `<-` can be annoying to type, so use RStudio’s keyboard short cut: 
Alt + - (the minus sign) to make life easier. 

Many people ask why we use this assignment operator when we can use `=` instead?

[Colin Fay had a Twitter thread on this subject](https://twitter.com/_colinfay/status/1006139974377443328),
but the reason I favour most is that it provides clarity. The arrow points in 
the direction of the assigment (it is actually possible to assign in the other
direction too) and it distinguishes between creating an object in the workspace 
and assigning a value inside a function.

Object name style is a matter of choice, but must start with a letter and can 
only contain letters, numbers, `_` and `.`. We recommend using descriptive names
and using `_` between words. Some special symbols cannot be used in variable 
names, so watch out for those.

So here we've used the name to indicate its value represents a mass in kilograms. 
Look in your environment pane and you'll see the `mass_kg` object 
containing the (data) value 55.

We can inspect an object by typing it's name:

```{r mass-inspect}
mass_kg
```

What's wrong here?

```{r mass-wrong-name, eval=FALSE}
mass_KG
```
`Error: object 'mass_KG' not found`

This error illustrates that typos matter, everything must be precise and `mass_KG`
is not the same as `mass_kg`. `mass_KG` doesn't exist, hence the error.

### Function anatomy {#function-anatomy}

Functions in R are objects followed by parentheses, such as `library()`.

Functions have the form:

`function_name(arg1 = val, arg2 = val2, ...)`

The use of arguements or inputs allows us to generalise. That is to say not
just do something in a specific case, but in many cases. For example not just
make a scatter plot for the `mpg` dataset, but for any dataset of observations
that can be plotted pairwise.

Let's use `seq()` to create a **seq**uence of numbers, and at the same time practice tab completion.

Start typing `se` in the console and you should see a list of functions appear,
add `q` to shorten the list, then use the up and down arrow to highlight the function
of interest `seq()` and hit Tab to select.

RStudio puts the cursor between the parentheses to prompt us to enter some 
arguments. Here we'll use 1 as the start and 10 as the end:

```{r sequence-function}
seq(1,10)
```

If we left off a parentheses to close the function, then when we hit enter 
we'll see a `+` indicating RStudio is expecting further code. We either add the 
missing part or press Escape to cancel the code.

Let's call a function and make an assignment at the same time. Here we'll use
the base R function `seq()` which takes three arguments: `from`, `to` and `by`.

Read the following code as *"make an object called my_sequence that stores a sequence of numbers from 2 to 20 by intervals of 2*.

```{r sequence-object}
my_sequence <- seq(2,20,2)
```

This time nothing was returned to the console, but we now have an object called
`my_sequence` in our environment.

Can you remember how to inspect it?

If we want to subset elements of `my_sequence` we use 
square brackets `[]`.

For example element five would be subset by:

```{r sequence-element}
my_sequence[5]
```

Here the number five is the index of the vector, not the value of the fifth element. The value of the fifth element is 10. 

And returning multiple elements uses a colon `:`, like so

```{r sequence-range}
my_sequence[5:8]
```

### Atomic vectors {#atomics}

We actually made an atomic vector already when we made `my_sequence`. We made a
a one dimensional group of numbers, in a sequence from two to twenty.

We're not going to be working much with atomic vectors in this workshop, 
but to make you aware of how R stores data, atomic vector types 
are: 

+ Doubles: regular numbers, +ve or -ve and with or without decimal places. AKA numerics.
+ Integers: whole numbers, specified with an upper-case L, e.g. `int <- 2L`
+ Characters: Strings of text
+ Logicals: these store `TRUE`s and `FALSE`s which are useful for comparisons.
+ Complex: this would be a vector of numbers with imaginary terms. 
+ Raw: these vectors store raw bytes of data.

Let's make a character vector and check the type:

```{r cards-characters}
cards <- c("ace", "king", "queen", "jack", "ten")

cards

typeof(cards)
```

### Attributes 

An attribute is a piece of information you can attach to an object, such as
names or dimensions. Attributes such as dimensions are added when 
we create an object, but others such as names can be added.

Let's look at the `mpg` data frame dimensions:

```{r attributes}
# mpg has 234 rows (observations) and 11 columns (variables)
dim(mpg)
```

### Factors

Factors are Rs way of storing categorical information such as eye colour or
car type. A factor is something that can only have certain values, and can be
ordered (such as `low`,`medium`,`high`) or unordered such as types of fruit.

Factors are useful as they code string variables such as "red" or "blue" to integer values e.g. 1 and 2, which can be used in statistical models and when plotting, but they are confusing as they look like strings. 

**Factors look like strings, but behave like integers.**

Historically R converts strings to factors when we load and create data, but 
it's often not what we want as a default. Fortunately, in the tidyverse strings
are not treated as factors by default.

### Lists

Lists also group data into one dimensional sets of data. The difference being 
that list group objects instead of individual values, such as several atomic 
vectors.

For example, let's make a list containing a vector of numbers and a character
vector

```{r list-example}
list_1 <- list(1:110,"R")

list_1
```

Note the double brackets to indicate the list elements, i.e. element one is the
vector of numbers and element two is a vector of a single character.

We won't be working with lists in this workshop, but they are a flexible way to 
store data of different types in R. 

Accessing list elements uses double square brackets syntax, for example
`list_1[[1]]` would return the first vector in our list. 

And to access the first element in the first vector would combine double and
single square brackets like so: `list_1[[1]][1]`.

Don't worry if you find this confusing, everyone does when they first start with
R.

### Matrices and arrays

Matrices store values in a two dimensional array, whilst arrays can have n 
dimensions. We won't be using these either, but they are also valid R objects.

### Data frames

Data frames are two dimensional versions of lists, and this is form of storing
data we are going to be using. In a data frame each atomic vector type becomes
a column, and a data frame is formed by columns of vectors of the same length.
Each column element must be of the same type, but the column types can vary.

Figure \@ref(fig:df) shows an example data frame we'll refer to as
saved as the object `df` consisting of three rows and three columns. Each 
column is a different atomic data type of the same length.

(ref:df) An example data frame `df`.


```{r df,fig.cap='(ref:df)', fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/data_frame.png")
```

Packages in the  tidyverse create a modified form of data frame called a tibble.
You can read about tibbles [here](http://r4ds.had.co.nz/tibbles.html). One 
advantage of tibbles is that they don't default to treating strings as factors. 
We deal with modifying data frames when we work with our example data set.

Sub-setting data frames can also be done with square bracket syntax, but as we
have both rows and columns, we need to provide index values for both row and 
column.

For example `df[1,2]` means **return the value of `df` row 1, column 2**. This corresponds with the value `A`. 

We can also use the colon operator to choose
several rows or columns, and by leaving the row or column blank we return all 
rows or all columns.

```{r subset-df, eval=FALSE}
# Subset rows 1 and 2 of column 1
df[1:2,1]

# Subset all rows of column 3
df[,3]
```

Again don't worry too much about this for now, we won't be doing to much of this
in this lesson, but it's important to be aware of the basic syntax.

## Learning more R

There are many places to start, but swirl can teach you interactively, and at your own pace in RStudio.

Just follow the instructions via this link: http://swirlstats.com/students.html

*Hands-On Programming with R* by Garrett Grolemund is another great resource 
for learning R.

Plus all the [tidyverse links](https://www.tidyverse.org/learn/).

<!--chapter:end:02-tidyverse.Rmd-->

# Creating scripts and importing data {#import}

Our analysis is of an example data set of observations for 7702 proteins
from cells in three control experiments and three treatment experiments.
The observations are signal intensity measurements from the mass spectrometer.
These intensities relate the concentration of protein observed in each experiment 
and under each condition.

We consider raw data as the data as we receive it. This doesn't mean it hasn't be
processed in some way, it just means it hasn't been processed by us.
Generally speaking we don't change the raw data file, what we do is import it and
create an object in R which we then transform.

So let's understand how to import some data.

## Some definitions

 + **Importing** means getting data into our R environment by creating an object 
 that we can then manipulate. The raw data file remains unchanged.
 + **Inspecting** means looking at the dataset to understand what it contains.
 + **Tidying** refers to getting data into a consistent format that makes it easy 
 to use in later steps.

### Rectangular data and flat formats {#file-formats}

Two further things to note:
 
 1.  Here we are only considering **rectangular data**, the sort that comes
 in rows and columns such as in a spreadsheet. Lots of our data types exist,
 such as images, but can also be
 handled by R. As mentioned in \@ref(biocondutor) genomic data in particular has 
 led to a project called [Bioconductor](http://bioconductor.org/) for the 
 development of analysis tools primarily in R, many of which deal with 
 non-rectangular data, but this is beyond the scope here.
 
 2. **Flat formats** are files that only contain plain text, with each line
 representing a set of observations and the variables separated by delimiters
 such as tabs, commas or spaces. Therefore there aren't multiple tables such
 as we'd get in an Excel file, or meta-data such as the colour highlighting of
 a cell in an Excel file. The advantages of flat files is that they can be
 opened and used by many different computing languages or programs. 
 So unless there is a good reason not to use a flat format, and there are good 
 reasons, they are the best way to store data in many situations.
 
## Using scripts

Using the console is useful, but as we build up a workflow, that is to say,
writing code to:

+ load packages 
+ load data
+ explore the data 
+ and output some results

Then it's much more useful to contain this in a script: a document of our code.

Why? When we write and save our code in scripts, we can re-use it, share it or 
edit it. But **most importantly a script is a record**.

Cmd/Ctrl + Shift + N will open a new script file up and you should see something
like Figure \@ref(fig:script-pane) with the script editor pane open:

(ref:script-pane) Rstudio with the script editor pane open.

```{r script-pane, fig.cap = '(ref:script-pane)', fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/rstudio_screenshot_four_panes.PNG")
```

## Running code

We can run a highlighted portion of code in your script if you click the Run
button at the top of the scripts pane as shown in Figure \@ref(fig:run-script). 

(ref:run-script) Scripts can be run by clicking the Source button.

```{r run-script, fig.cap= '(ref:run-script)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/run_script.PNG")
```

You can run the entire script by clicking the Source button.

Or we can run chunks of code if we split our script into sections, see below.

## Creating a R script

We first need to create a script that will form the basis of our analysis.

Go to the file menu and select New Files > R script. This should open the script
editor pane.

Now let's save the script, by going to File > Save and we should find ourselves
prompted to save the script in our Project Directory.

Following the advice about [naming things](#names) we can create a new R script
called `01-bspr-workshop-july-2018`.

This name is machine readable (no spaces or special characters), human readable,
and works well with default ordering by beginning with `01`.

## Setting up our environment

At the head of our script it's common to put a title, the name of the author
and the date, and any other useful information. This is created as comments 
using the `#` at the start of each line.

It's then usual to follow this by code to load the packages we need into our
our R environment using the `library()` function and providing the name of the
package we wish to load. Packages are collections of R functions.

Often we break the code up into regions by adding dashes (or equals symbols) 
to the comment line. This enables us to run chunks of the script separately from
running the whole script when using our code.

Here is a typical head for a script:

```{r set-up, eval=FALSE}
# My workshop script
# 7th July 2018
# Alistair Bailey

# Load packages ----------------------------------------------------------------
library(plyr)
library(tidyverse)
library(gplots)
library(pheatmap)
library(gridExtra)
library(VennDiagram)
library(ggseqlogo)
```

### Bioconductor {#biocondutor}

As an aside there are many proteomics specific R packages, these are generally
found through [Bioconductor](https://www.bioconductor.org/) which is a project
that was initiated in 2001 to create tools for the analysis of high-throughput 
genomic data, but also includes other 'omics data tools [@gentleman2004,@huber2015].

Exploring Bioconductor is beyond our scope here, but well worth exploring for
manipulation and analysis of raw data formats such as mzxml files.

## Importing data

Assuming our data is in a [flat format](#file-formats), we can import it into our 
environment using the tidyverse `readr` package. 

If our data was an excel file, we can use the tidyverse `readxl` package to 
import the data, but it will remove any meta-data and each table in the excel
file will become a separate R object as per tidy data principles.

For the purposes of this workshop we have a `csv` (comma separated variable) file.

If you haven't done so already
<a href="https://raw.githubusercontent.com/ab604/ab604.github.io/master/docs/070718-proteomics-example-data.csv" download>Click here to download</a> the example data and save it to our project 
directory. Check the `Files` pane to see it's there.

We then import data and assign it to an object we'll call `data` like so:

```{r example-data, echo=FALSE, eval=F,cache=T}
# Charlotte's data
dat <- read_csv("data/MergedFinalProt_2colPerFile_12files_filtered_ch.csv",
                   na = "NaN", col_types = cols()) %>% clean_names()

# Clean it up for workshop, keep missing values
dat_select <- dat %>%
#   # Select and rename columns
  select(protein_accession = protein_entry_protein_accession,
         protein_description,
         control_1 = c1,
         control_2 = c2,
         control_3 = c3,
         treatment_1 = rt1,
         treatment_2 = rt2,
         treatment_3 = rt3)

# Write out the example data
write_csv(dat_select,"data/070718-proteomics-example-data.csv")
```

```{r import-data, eval=TRUE, cache=F}
# Import example data ----------------------------------------------------------
# Import the example data with read_csv from the readr package
dat <- readr::read_csv("data/070718-proteomics-example-data.csv")
```

## Exploring the data

### `glimpse`, `head` and `str`

The first thing to do with any data set is to actually look at it. Here are
four ways to have look at the data in the `Console`: calling the object directly, 
`glimpse`, `head` and `str`.

1. We can just call the object and return it to the `Console`, which may or may
not be useful depending on the size and type of object we call.

2 .`glimpse` is a tidyverse function that tries to show us as much data in
a data.frame or tibble as possible, telling us the [atomic types](#atomics) of 
data in the table, the number of observations and the number of variables, and 
importantly shows all the column variable names by transposing the table.

3. `head` is a base function that shows us the 6 lines of a R object by default.

4. `str` is a base function that show the structure of a R object, so it provides
a lot of information, but is not so easy to read.

The outputs for these four functions is shown below:

```{r examine-data}
# call object
dat

# tidyverse glimpse function
glimpse(dat)

# head function
head(dat)

# str function
str(dat)
```

To see the data in a *spreadsheet* fashion use `View(dat)`, note the capital V
and a new tab will open. This can also be launched from the `Environment` tab by
clicking on `dat`.

Although this provides us with some useful information, such as the number of
observations and variables, to understand more plotting the data will be
helpful as we'll see in Section \@ref(normalisation).

### Summary statisitics

Another useful way to quickly get a sense of the data is to use the `summary`
function, which will return summary of the spread of the data and importantly
if there are missing values. We can see immediately below that the experimental
replicates have different distributions, and missing values that we need to 
deal with in Chapter \@ref(transform).

```{r summary-stats}
summary(dat)
```

<!--chapter:end:03-importing.Rmd-->

# `dplyr` verbs and piping {#dplyr}

A core package in the tidyverse is `dplyr` for transforming data, which is often
used in conjunction with the `magrittr` package that allows us to pipe multiple
operations together.

The R4DS dplyr chapter is [here](http://r4ds.had.co.nz/transform.html) and for
magrittr [here](http://r4ds.had.co.nz/pipes.html).

The figures in this chapter we made for use with an ecological dataset on rodent
surveys, but the principles they illustrate are generic and show the use of 
each function with or without the use of a pipe.

From R4DS:

"*All `dplyr` verbs work similarly:*

*1. The first argument is a data frame.*

*2. The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).*

*3. The result is a new data frame.*

*Together these properties make it easy to chain together multiple simple steps to achieve a complex result.*"

## Pipes {#pipes}

A pipe in R looks like this `%>%` and allows us to send the output of one 
operation into another. This saves time and space, and can make our code easier
to read.

For example we can pipe the output of calling the `dat` object into the `glimpse`
function like so: 

```{r pipe-glimpes, eval=T}
dat %>% glimpse()
```

This becomes even more useful when we combine pipes with `dplyr` functions.

## Filter rows {#filter}

The `filter` function enables us to filter the rows of a data frame according to
a logical test (one that is `TRUE` or `FALSE`). Here it filters rows in
the surveys data where the year variable is greater or equal to 1985. 

```{r filter,fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/dplyr_filter.png")
```

Let's try this with `dat` to filter the rows for proteins in `control_1` and 
`control_2` experiments where the observations are greater than 20:

```{r filter-dat}
dat %>% filter(control_1 > 20, control_2 > 20)
```

Filtering is done with the following operators `>`,`<`,`>=`,`<=`,`!=` (not equal)
and `==` for equal. Not the double equal sign.

## Arrange rows

Arranging is similar to filter except that it changes the row order according
to the columns in ascending order. If you provide more than one column name, 
each additional column will be used to break ties in the values of preceding 
columns.

Here we arrange the surveys data according to the record identification number.

```{r arrange,fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/dplyr_arrange.png")
```

To try that with `dat` let's arrange the data according to `control_1`:

```{r dat-arrange}
dat %>% arrange(control_1)
```

## Select columns

Selecting is the verb we use to select columns of interest in the data. Here
we select only the `year` and `plot_type` columns and discard the rest.

```{r select,fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/dplyr_select.png")
```

Let's use select with `dat` to drop the protein description and control 
experiments using negative indexing and keep everything else:

```{r dat-select}
dat %>% select(-protein_description,-(control_1:control_3))
```

## Create new variables {#mutate}

Creating new variables uses the `mutate` verb. Here I am creating a new 
variable called `rodent_type` that will create a new column containing the type
of rodent observed in each row.

```{r mutate,fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/dplyr_mutate.png")
```

Let's create a new variable for `dat` called `prot_id` that use the `str_extract`
function from the `stringr` package to take the last 6 characters of
the `protein_accession` variable, the `".{6}$"` part is called a regular 
expression, to keep just the UNIPROT id part of the string.

We'll use select to drop the other variables except the 
protein accession afterwards via another pipe.

```{r dat-mutate}
dat %>% 
  mutate(prot_id = str_extract(protein_accession,".{6}$")) %>% 
  select(protein_accession, prot_id)
```

## Create grouped summaries

The last key verb is `summarise` which collapses a data frame
into a single row. 

For example, we could use it to find the average weight
of all the animals surveyed in the surveys data using `mean()`. 
(Here the `na.rm = TRUE` argument is given to remove missing values from the 
data, otherwise R would return `NA` when trying to average.)

```{r summarise,fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/dplyr_summarise.png")
```

`summarise` is most useful when paired with `group_by`
which defines the variables upon which we operate upon. 

Here if we group by `species_id` and `rodent_type` together and then used 
`summarise` without any arguments we return these two variables only.

```{r group-by,fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/dplyr_group_by.png")
```

We'll use the `mpg` dataset again to illustrate a grouped summary. Here
I'll group according fuel type `fl`, c = compressed natural gas ,d = diesel, 
e = ethanol, p = premium and r = regular.
Then using summarise to calculate the mean highway (`hwy`) miles per gallon,
and the mean urban (`cty`) miles per gallon,the tables is collapsed from 234 to 
five rows, one for each fuel type and two columns for the mean mpg;s. This
illustrates how grouped summaries provide a very concise way of exploring data
as we can immediately see the relative fuel efficiences of  each fuel type under
two conditions.

```{r dat-group_mean}
# fl is fuel type. c = compressed natural gas ,d = diesel, 
# e = ethanol, p = premium and r = regular.
mpg %>% 
  group_by(fl) %>% 
  # Create summaries mean_hwy and mean_cty using the mean function, 
  # dropping any missing variables.
  summarise(mean_hwy = mean(hwy, na.rm = T), mean_cty = mean(cty, na.rm = T))
```

We'll use `dplyr` and pipes in Chapter \@ref(transform).

<!--chapter:end:04-dplyr.Rmd-->

# Transforming and visualising proteomics data {#transform}

Having imported our data set of observations for 7702 proteins
from cells in three control experiments and three treatment experiments.
Remember, the observations are signal intensity measurements from the mass 
spectrometer, and these intensities relate to the amount of protein in each 
experiment and under each condition.

Now we will transform the data to examine the effect of
the treatment on the cellular proteome and visualise the output using a volcano
plot and a heatmap. The hypothesis we are testing is that treatment
changes the concentration of protein we observe.

A volcano plot is commonly used way of plotting changes in observed values
on the x-axis against the likelihood of observing that change due to chance
on the y-axis. Heatmaps are another way of visualising the relative 
(increase and decrease of) amounts of observed values.

## Fold change and log-fold change

Fold changes are ratios, the ratio of say protein expression before and
after treatment, where a value larger than 1 for a protein implies that 
protein expression was greater after the treatment.

In life sciences, fold change is often reported as log-fold change. Why is that?
There are at least two reasons which can be shown by plotting.

One is that ratios are not symmetrical around 1, so it's difficult to observe
both changes in the forwards and backwards direcion i.e. proteins where expression
went up and proteins where expression went down due to treatment. When we 
transform ratios on a log scale, the scale becomes symmetric around 0 and thus
we can now observe the distribution of ratios in terms of positive, negative or
no change.

(ref:logratios) Ratios are not symmetric around one, logratios are symmetric around zero.

```{r fold-change-1,fig.cap='(ref:logratios)', echo=FALSE, cache=TRUE}
set.seed(10)
x <- 2^(rnorm(100))
y <- 2^(rnorm(100))
ratios <- tibble(value = x/y, label = "ratios")
logratios <- tibble(value = log2(ratios$value), label = "logratios")


bind_rows(ratios,logratios) %>% 
  mutate(label = factor(label, levels = c("ratios","logratios"))) %>% 
  ggplot(aes(value)) +
  geom_histogram(binwidth = 2, colour = "white", fill = "red") +
  ggplot2::facet_wrap(~ label) +
  xlab("") +
  ylab("") +
  theme_minimal()
```

A second reason is that transforming values onto a log scale changes where
the numbers actually occur when plotted on that scale. If we consider the log
scale to represent magnitudes, then we can more easily see changes of small and
large magnitudes when we plot the data.

For example, a fold change of 32 times can be either a ratio 1/32 or 32/1. 

As shown in Figure \@ref(fig:fold-change-2), 1/32 is much closer to 1 than 32/1, 
but transformed to a log scale we see that in terms of magnitude of difference it 
is the same as 32/1.

Often the log transformation is to a base of 2 as each increment of 1 represents
a doubling, but sometimes a base of 10 is used, for example for p-values.

(ref:logratio-2) Transformation of scales using log transformation.

```{r fold-change-2, fig.cap='(ref:logratio-2)', echo=FALSE, cache=TRUE}
x2 <- 2^seq(1,5)
y_vals <- c(rev(1/x2),1,x2)
names <- c(paste0("1/",rev(x2)),1,x2)
x_vals <- seq(along=y_vals)

sim_dat <- tibble(x_vals,y_vals,names)

p1 <- ggplot(sim_dat,aes(x_vals,y_vals, label = names)) +
  geom_text() +
  geom_hline(yintercept = 1) +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = NULL)


p2 <- ggplot(sim_dat,aes(x_vals,y_vals, label = names)) +
  geom_text() +
  geom_hline(yintercept = 1) +
  scale_y_continuous(trans = "log2") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = NULL)

plot_grid(p1,p2)
```

## Dealing with missing values {#missing-values}

<!---
1. Load the data, we need multiple replicates for each condition. To be tidy
each protein is a set of observations (rows) of the variables, which are the
values recorded for each replicate (columns).

2. Tidy up and deal with missing values, either impute or exclude missing values
and normalise. --->

Unless we're really lucky, it's unlikely that we'll get observations for
the same numbers of proteins in all replicated experiments. This means
there will be missing values for some proteins when looking at all the 
experiments together. This then raises the question of what to do about the
missing values? We have two choices:

1. Only analyse the proteins that we have observations for in all experiments.
2. Impute values for the missing values from the existing observations.

There are pros and cons to either approach. Here for simplicity we'll use only
the proteins for which we have observations in all assays.

We can drop the proteins with missing values by piping our data set to the
`drop_na()` function from the `tidyr` package like so. We assign this to a new
object called `dat_tidy`.

We'll use the summarise function to compare the number of proteins before and
after dropping the missing values using the `n()` counting function.

```{r missing values, cache=F}
# Remove the missing values
dat_tidy <- dat %>% drop_na()
# Nunber of proteins in original data
dat %>% summarise(Number_of_proteins = n())
# Nunber of proteins without missing values
dat_tidy %>% summarise(Number_of_proteins = n())
```

This shrinks the dataset from 7,702 proteins to 1,145 proteins, so we can see 
why imputing the missing values might be more atrractive.

One approach you might like to try is to impute the data by replacing the 
missing values with the mean observation for each protein under each condition.

## Data normalization {#normalisation}

To perform statistical inference, for example whether treatment increases or
decreases protein abundance, we need to account for the variation that occurs
from run to run on our spectrometers and each give rise to a different 
distribution. This is as opposed to variation arising from treatment versus
control which we are interested in understanding. Hence normalisation seeks to 
reduce the run-to-run sources of variation.

A method of normalization introduced for DNA microarray analysis is 
quantile normalisation [@bolstad2003]. There are various ways to normalise data,
so using quantile normalisation here is primarily to demonstate the approach in
R, you should consider what is best for your data.

If we consider our proteomics data as a distribution of values, one value for 
the concentration of each protein in our experiment that together form a 
distribution. Figure \@ref(fig:data-dist) shows the distribution of
protein concentrations observed for the three control and three treatment assays.
As we can see the distributions are different for each assay.

(ref:prot-dist) Protein data for six assays plotted as a distributions.

```{r data-dist, fig.cap='(ref:prot-dist)',fig.asp=0.5, out.width= '80%', fig.align='center', echo=FALSE, cache=TRUE}
# Plot data
d1 <- dat_tidy %>%
  gather(experiment,value,-c(1:2)) 

d1 %>%
  ggplot(aes(log2(value),colour = experiment)) +
  geom_density() +
  xlab("") +
  theme_minimal()
```

A quantile represents a region of distribution, for example the 0.95 quantile
is the value such that 95% of the data lies below it. To normalize two or more
distributions with each other without recourse to a reference distribution we:

(i) Rank the value in each experiment (represented in the columns) from 
lowest to highest. In other words identify the quantiles for each protein 
in each experiment.
(ii) Sort each experiment (the columns) from lowest to highest value.
(iii) Calculate the mean across the rows for the sorted values.
(iv) Then substitute these mean values back according to rank for each experiment
to restore the original order.

This results in the highest ranking observation in each experiment
becoming the mean of the highest observations across all experiments, the
second ranking observation in each experiment becoming the mean of the 
second highest observations across all experiments. Therefore the 
distributions for each each experiment are now the same.

[Dave Tang's Blog:Quantile Normalisation in R](https://davetang.org/muse/2014/07/07/quantile-normalisation-in-r/) has more
details on this approach.

(ref:quant-norm) Quantile Normalisation from [Rafael Irizarry's tweet](https://twitter.com/rafalab/status/545586012219772928?ref_src=twsrc%5Etfw).

```{r quant-norm, fig.cap='(ref:quant-norm)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/quant_norm.png")
```

These result of quantile normalisation is that our distributions become
statisitcally identitical, which we can see by plotting the densities of the
normalized data. As shown in Figure \@ref(fig:compare-normalisation) the distributions
all overlay.

We do this by creating a [function](#function-anatomy). This takes a data
frame as the arguement and pefrorms the steps described to iterate through
the data frame. 

The code below is probably quite tricky to understand if
you've not seen `map` functions before, but they enable a function such as `rank`
or `sort` to be used on each column iteratively. What's important here is
to understand the aim, even if understanding the code requires some more reading.
You can read about [map functions in R4DS](http://r4ds.had.co.nz/iteration.html#the-map-functions).

```{r quant-normalisation, cache=TRUE}
# Quantile normalisation : the aim is to give different distributions the
# same statistical properties
quantile_normalisation <- function(df){
  
  # Find rank of values in each column
  df_rank <- map_df(df,rank,ties.method="average")
  # Sort observations in each column from lowest to highest 
  df_sorted <- map_df(df,sort)
  # Find row mean on sorted columns
  df_mean <- rowMeans(df_sorted)
  
  # Function for substiting mean values according to rank 
  index_to_mean <- function(my_index, my_mean){
    return(my_mean[my_index])
  }
  
  # Replace value in each column with mean according to rank 
  df_final <- map_df(df_rank,index_to_mean, my_mean=df_mean)
  
  return(df_final)
}
```

The normalisation function is used by piping `dat_tidy` first to `select` to 
exclude the first two columns with the protein accession and description in, and
then to the normalisation function. We re-bind the protein accession and description afterwards from `dat_tidy` by piping the output to `bind_cols()`. 

```{r qnorm-data, cache=F}
dat_norm <- dat_tidy %>% select(-c(protein_accession:protein_description)) %>% 
  quantile_normalisation() %>% 
  bind_cols(dat_tidy[,1:2],.)
```

(ref:compare-qnorm) Comparison of the protein distributions before normalization (left)
and after quantile normalization (right).

```{r compare-normalisation, fig.cap='(ref:compare-qnorm)', out.width= '80%', fig.asp= 0.5, fig.align='center', echo = FALSE, cache=F}
# Plot normalised data
d2 <- dat_norm %>%
  gather(key = experiment,value,-c(1:2)) %>% 
  ggplot(aes(log2(value),colour = experiment)) +
  geom_density() +
  xlab("") +
  theme_minimal() +
  theme(legend.position="none") +
  ylim(0,0.28) +
  xlim(-10,5)
  

d1c <- d1 %>%
  ggplot(aes(log2(value),colour = experiment)) +
  geom_density() +
  xlab("") +
  theme_minimal() + 
  theme(legend.position="none") + ylim(0,0.28)

plot_grid(d1c,d2)
```

## Hypothesis testing with the t-test

Having removed missing values and normalised the data, we can consider our
hypothesis: treatement changes the amount of protein we observe in the cells.

In practice then, what we would like to know is whether the mean value for each
protein in our control and treatment assays differs due to chance or due a 
real effect. We therefore need to calculate the difference for each protein
between treatment and control, and the probability that any difference occurs 
due to chance. This is what the p-value from the output of a 
t-test seeks to do. We need to perform 1145 t-tests.

**Note** There are biocondutor packages that contain functions written to do
this. However as a learning exercise we are going to work through the problem.

Here I assume the reader is familiar with t-tests, but just to re-cap some
important points:

+ We assume that the true population from which our data samples are 
indpendent, identically distributed and follow a normal distribution. This is 
not in fact true in practice, but t-test is robust to this assumption.

+ We assume unequal variances between the control and treatment for each protein. Hence we will perform a Welch's t-test for unequal variances.

+ We don't know whether the effect of the treatment is to increase or decrease
the concentration of the protein, hence we will perform a two-sided t-test.

+ The observations for the proteins are for proteins of the same type but from 
independent experiments, rather than observations of the same individuals
before and after treatment. Hence we test the observations as unpaired samples.

In R we use the base function `t.test` to perform Welch Two Sample t-test and
this outputs the p-values we need for each protein. However, the challenge here
is that our data has three observations for each condition for each protein,
hence we need to group the observations for each protein according to the 
experimental condition as inputs to each t-test.

We're going to follow what is called the *split-apply-combine* approach to deal 
with this problem:

1. Split the data into control and treatment groups.
2. Apply the t-test function to each protein using the grouped inputs and
store the p-value.
3. Combine all the p-values for each protein into a single vector.

To this end I've created a function called `t_test` that takes a data frame
and two group vectors as inputs. It splits the data into `x` and `y` by 
subsetting the the data frame according to the columns defined by the groups. 
The extra steps here are that the subset data has to be unlisted and converted 
to numeric type for input to the `t.test` function. We then perform the t-test, 
which will calculate the mean of `x` and `y` and store the result in a new 
object, and finally the function creates a data frame with a single variable`p_val` which is then returned as the function output.

```{r t-test-function, cache=TRUE}
# T-test function for multiple experiments
t_test <- function(dt,grp1,grp2){
  # Subset control group and convert to numeric
  x <- dt[grp1] %>% unlist %>% as.numeric()
  # Subset treatment group and convert to numeric
  y <- dt[grp2] %>% unlist %>% as.numeric()
  # Perform t-test using the mean of x and y
  result <- t.test(x, y)
  # Extract p-values from the results
  p_vals <- tibble(p_val = result$p.value)
  # Return p-values
  return(p_vals)
} 
```

To use the `t_test` function to perform many t-tests and not just one t-test, we 
need to pass our `t_test` function as an arguement to another function.

This probably seems quite confusing, but the point here is that we want to loop
through every row in our table, and group the three control and three treatment
columns separately. Our `t_test` function deals with the latter problem, and 
by passing it to `adply` from the `plyr` package we can loop through each row
and it adds the calculated p-values to our original table.

Concretely then, `adply` takes an array and applies the `t_test` function to 
each row and we supply the column group indices arguments to the `t_test` 
funcition. Here the indicies are columns 3 to 5 for the control experiments and 
columns 6 to 8 for the treatment functions. The function returns the input
data with an additional corresponding p-value column. **Note** I've piped
the output to `as.tibble()` to transform the data.frame output of `adply` to
tibble form to prevent errors that can occur if we try to bind data frames
and tibbles.

An important point here is that we can use this function for any number of 
columns and rows providing our data is in the same tidy form by changing the
grouping indices.

```{r t-tests, cache=F}
# Apply t-test function to data using plyr adply
#  .margins = 1, slice by rows, .fun = t_test plus t_test arguements
dat_pvals <- plyr::adply(dat_norm,.margins = 1, .fun = t_test, 
                grp1 = c(3:5), grp2 = c(6:8)) %>% as.tibble()
```

To check our function, here's a comparision of calculating the first protein 
p-value as a single t-test as shown in the following code and the output of
the function.

```{r, eval=FALSE}
# Perform t-test on first protein
t.test(as.numeric(dat_norm[1,3:5]),
                    as.numeric(dat_norm[1,6:8]))$p.value
```

```{r pval-table,echo=FALSE}
# Calculate the p-value for the first protein and compare with function
p_check <- tibble(p1= round(dat_pvals$p_val[1],4),
                  p2 =round(t.test(as.numeric(dat_norm[1,3:5]),
                    as.numeric(dat_norm[1,6:8]))$p.value,4))

knitr::kable(p_check %>% select("t_test p-val" = p1, "t.test p-val" = p2),
  booktabs=TRUE)
```

We can plot a histogram of the p-values:

```{r p-val-plot}
# Plot histogram
dat_pvals %>% 
  ggplot(aes(p_val)) + 
  geom_histogram(binwidth = 0.05, 
           boundary = 0.5, 
           fill = "darkblue",
           colour = "white") +
  xlab("p-value") +
  ylab("Frequency") +
  theme_minimal()
```

## Calculating fold change

To perform log transformation of the observations for each protein we take our
data and use select to exlude the columns of character vectors and the pipe the
output to `log2()` and use the pipe again to create a data frame.

Then we use `bind_cols` to bind the first two columns of `dat_pvals` followed
by `dat_log` and the last column of `dat_pvals`. This maintains the original
column order.

```{r log-data, cache=F}
# Select columns and log data
dat_log <- dat_pvals %>% 
  select(-c(protein_accession,protein_description,p_val)) %>% 
  log2()

# Bind columns to create transformed data frame
dat_combine <- bind_cols(dat_pvals[,c(1:2)], dat_log, dat_pvals[,9]) 
```

The log fold change is then the difference between the log mean control and 
log mean treatment values. By use of grouping by the protein accession we
can then use `mutate` to create new variables that calculate the mean values 
and then calculate the `log_fc`. Whilst we're about it, we can also calculate 
a -log10(p-value). As with fold change, transforming the p-value on a log10
scale means that a p-value of 0.05 or below is transformed to 1.3 or above and
a p-value of 0.01 is equal to 2.

```{r mean-log, cache=F}
dat_fc <- dat_combine %>% 
  group_by(protein_accession) %>% 
  mutate(mean_control = mean(c(control_1,
                               control_2,
                               control_3)),
                             mean_treatment= mean(c(treatment_1,
                                                    treatment_2,
                                                    treatment_3)),
         log_fc = mean_control - mean_treatment,
         log_pval = -1*log10(p_val))
```

The next step is not necessary, but for ease of viewing we subset `dat_fc` to 
create a new data frame called `dat_tf` that contains only four variables. 
We could potentially write this to a csv file for sharing.

```{r dat-tf}
# Final transformed data
dat_tf <- dat_fc %>% select(protein_accession,
                            protein_description,
                            log_fc, log_pval)
```

Let's look at the head of the final table:
```{r dat-tf-table, echo=F}
# Create a table of the d
knitr::kable(dat_tf %>% head(.,5),
  booktabs=TRUE)
```

## Visualising the transformed data

Plotting a histogram of the log fold change gives an indication of whether the 
treatment has an effect on the cells. Most values are close to zero, but there
are some observations far above and below zero suggesting the treatment 
does have an effect.

(ref:log-hist) Histogram of log fold change.

```{r log-fc, fig.cap='(ref:log-hist)',fig.asp=1, out.width= '80%', fig.align='center',cache=F}
# Plot a histogram to look at the distribution.
dat_tf %>%
  ggplot(aes(log_fc)) + 
  geom_histogram(binwidth = 0.5,
                 boundary = 0.5,
           fill = "darkblue",
           colour = "white") +
  xlab("log2 fold change") +
  ylab("Frequency") +
  theme_minimal()
```

However, we don't know if these fold changes are dueto chance or not, which is 
why we calculated the p-values. A volcano plot will include the p-value 
information.

## Volcano plot

<!--- 7. Create a combined table of log fold change and p-values for all the proteins
for plotting a volcano plot. --->

A volcano plot is a plot of the log fold change in the observation between two
conditions on the x-axis, for example the protein expression between treatment 
and control conditions. On the y-axis is the corresponding p-value for each
observation, representing the likelihood that an observed change
is due to the different conditions rather than arising from a natural variation 
in the fold change that might be observed if we performed many replications of 
the experiment.

The aim of a volcano plot is to enable the viewer to quickly see the effect
(if any) of an experiment with two conditions on many species (i.e. proteins)
in terms of both an increase and decrease of the observed value.

Like all plots it has it's good and bad points, namely it's good that we can
visualise a lot of complex information in one plot. However this is also it's 
main weakness, it's rather complicated to understand in one glance.

```{r volcano-plot, cache=F}
dat_tf %>% ggplot(aes(log_fc,log_pval)) + geom_point()
```

However it would be much more useful with some extra formatting, so the code
below shows one way to transform the data to include a threshold which can
then be used by ggplot to create an additional aesthetic. The code below
also includes some extra formatiing which the reader can explore.

(ref:vplot) A volcano plot with formatting to highlight the significant proteins

```{r nice-vplot, fig.cap='(ref:vplot)',fig.asp=1, out.width= '80%', fig.align='center'}
dat_tf %>%
  # Add a threhold for significant observations
  mutate(threshold = if_else(log_fc >= 2 & log_pval >= 1.3 |
                               log_fc <= -2 & log_pval >= 1.3,"A", "B")) %>%
  # Plot with points coloured according to the threshold
  ggplot(aes(log_fc,log_pval, colour = threshold)) +
  geom_point(alpha = 0.5) + # Alpha sets the transparency of the points
  # Add dotted lines to indicate the threshold, semi-transparent
  geom_hline(yintercept = 1.3, linetype = 2, alpha = 0.5) + 
  geom_vline(xintercept = 2, linetype = 2, alpha = 0.5) +
  geom_vline(xintercept = -2, linetype = 2, alpha = 0.5) +
  # Set the colour of the points
  scale_colour_manual(values = c("A"= "red", "B"= "black")) +
  xlab("log2 fold change") + ylab("-log10 p-value") + # Relabel the axes
  theme_minimal() + # Set the theme
  theme(legend.position="none") # Hide the legend
```

### But which proteins are the significant observations?

To extract the proteins in red in Figure \@ref(fig:nice-vplot) we filter `dat_tf`
according to our threshold and then create a new variable using the `str_extract`
function used in Section \@ref(mutate).

**Note** We need to ungroup the data we grouped when calculating the log_fc to 
be able to select columns without keeping the grouping variable column too.

```{r sig-obs}
dat_tf %>%
  # Filter for significant observations
  filter(log_pval >= 1.3 & (log_fc >= 2 | log_fc <= -2)) %>% 
  # Get last six characters
  mutate(prot_id = str_extract(protein_accession,".{6}$")) %>% 
  # Ungroup the data
  ungroup() %>% 
  # Select columns of interest
  select(prot_id,protein_description,log_fc,log_pval)
```

## Creating a heatmap

Here we'll create a heatmap using the `heatmap.2` function from the `gplots`
package and the `pheatmap` function from the `pheatmap` package.

To create a heatmap we need to perform a few more transformations:

1. Filter the data according to a threshold of significance. This time we'll use
a more relaxed log_fc cut-off to ensure we have enough proteins to plot. At
the same time we'll extract the protein ids as before.

2. We then have to transform our filtered data into a `matrix.data.frame` object
for use with `pheatmap`. We name the rows with the protein ids

3. We'll use base R function `scale` to centre our log transformed data around zero. To do this per experiment we transpose the matrix as scale centres rows, 
and the flip the matrix back again.

<!-- 1. Clustering the data -->
```{r}
# Keep the same p-val cut-off, but relax the log_fc to 1 which represents a 
# doubling
dat_filt <- dat_fc %>%
  filter(log_pval >= 1.3 & (log_fc >= 1 | log_fc <= -1)) %>% 
  mutate(prot_id = str_extract(protein_accession,".{6}$"))

# Convert to matrix data frame
dat_matrix <- as.matrix.data.frame(dat_filt[,3:8]) 
# Name the rows with protein ids
row.names(dat_matrix) <- dat_filt$prot_id
# Transpose and scale the data to a mean of zero and sd of one
dat_scaled <- scale(t(dat_matrix)) %>% t()
```

### Calculating similarity and clustering

At this point we could just plot the data, but to understand what the heatmap
functions do to cluster the data, let's step through the process.

Our data here as log fold change in concentrations, but how do we group them? 
The simplest thing to do is to turn the data into distances, as a measure of 
similarity, where close things are similar and distant things are dissimilar.

The Euclidean distance $d$ between a pair of observations $x_i$ and $y_i$ is defined as:

$d = \sqrt{\sum{_i}(x_i - y_i)^2}$

Lets calculate the distance between the columns in `dat_scaled`. 

In `dat_scaled` the experiments are in the columns. In calculating the distance 
is between the experiments for all the proteins in each experiment. What would 
we expect?

We'd expect the controls to be close to each other and the treated to be close
to each other, right?

Let's do this in detail, for example the distance between `control_1` and
`control_2` is `sqrt(sum((dat_scaled[,1] - dat_scaled[,2])^2))`. 

This means we take the column 2 values from column 1 values, squaring the results
and summing them all to a single value and taking the square root to find the 
linear distance between these rows, which is `r round(sqrt(sum((dat_scaled[,1]-dat_scaled[,2])^2)),2)`. 

You can check  this against the first value in `d1` that we calculate below in using `dist`.

We do the same for the proteins, but we don't know what to expect. Here's the
code for calculating both distance matrices

```{r distance}
# Transpose the matrix to calculate distance between experiments, row-wise
d1 <- dat_scaled %>% t() %>%
  dist(.,method = "euclidean", diag = FALSE, upper = FALSE)
# Calculate the distance between proteins row-wise 
d2 <- dat_scaled %>%
  dist(.,method = "euclidean", diag = FALSE, upper = FALSE)

# Show the values for d1
round(d1,2)
```

Having calculated the distance matrices, we can cluster proteins and experiments 
accordingly.

There are lots of flavours of clustering, and no clear way to say which is best.
Here we'll use the Ward criterion for clustering which attempts to minimise the 
variance within clusters as it merges the data into clusters, using the distances
we've calculated. The data is merged from the bottom up (aka agglomeration) 
adding data points to a cluster and splitting them according to the variance 
criterion.

See Wikipedia for more detail:
[Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) 

```{r clustering}
# Clustering distance between experiments using Ward linkage
c1 <- hclust(d1, method = "ward.D2", members = NULL)
# Clustering distance between proteins using Ward linkage
c2 <- hclust(d2, method = "ward.D2", members = NULL)
```

Now lets look at the dendrograms made by
clustering our distance matrices `d1` and `d2`:

(ref:dend) Dendrograms of Ward clustering of distance matrices

```{r dendrograms, fig.cap='(ref:dend)',fig.asp=1, out.width= '80%', fig.align='center'}
# Check clustering by plotting dendrograms
par(mfrow=c(2,1),cex=0.5) # Make 2 rows, 1 col plot frame and shrink labels
plot(c1); plot(c2) # Plot both cluster dendrograms
```

As we'd expect, Figure \@ref(fig:dendrograms) shows the controls and treatments 
cluster respectively. 

<!-- 2. Plot the data -->

### Plotting the heatmap

The `heatmap.2` function from the `gplots` package will automatically perform
the distance calculation and clustering we performed, and it can also do
the scaling we did. It only requires the matrix as an input by default. It will
use a different clustering method by default.

However, as we've performed scaling and calculated the clusters, we can pass
them to heatmap function. 

I'll leave it to the reader to explore all the options here, but the concept in 
the code below to create Figure \@ref(fig:heatmap2) is:

+ Create  a 25 increment blue/white/red colour pallette
+ Pipe `dat_scaled` to a function that renames the colums
+ Pipe this to the `heatmap.2` function
+ Pass the clusters `c1` and `c2` to the plot
+ Change some aesthetics such as the colours, and the font sizes

(ref:heatmap2) Heatmap created with `heatmap.2` using the clusters calculated.

```{r heatmap2,fig.cap='(ref:heatmap2)',fig.asp=1, out.width= '80%', fig.align='center'}
# Set colours for heatmap, 25 increments
my_palette <- colorRampPalette(c("blue","white","red"))(n = 25)

# Plot heatmap with heatmap.2
par(cex.main=0.75) # Shrink title fonts on plot
dat_scaled %>% 
  # Rename the comlums
  magrittr::set_colnames(c("Ctl 1", "Ctl 2", "Ctl 3",
                                    "Trt 1", "Trt 2", "Trt 3")) %>% 
  # Plot heatmap
  gplots::heatmap.2(.,                     # Tidy, normalised data
          Colv=as.dendrogram(c1),     # Experiments clusters in cols
          Rowv=as.dendrogram(c2),     # Protein clusters in rows
          revC=TRUE,                  # Flip plot to match pheatmap
          density.info="histogram",   # Plot histogram of data and colour key
          trace="none",               # Turn of trace lines from heat map
          col = my_palette,           # Use my colour scheme
          cexRow=0.6,cexCol=0.75)     # Amend row and column label fonts
```

An alternative and more `ggplot` style is to use the `pheatmap` package and
function [@R-pheatmap].

In Figure \@ref(fig:pheatmap) `dat_scaled` is piped to `set_columns` again to 
rename the experiments for aesthetic reasons. The output is the piped to 
`pheatmap` which performs the distance and clustering automatically. The only 
additional arguements used here are to change the fontsize and create some 
breaks in the plot to highlight the clustering.

There is lots more that `pheatmap` can do in terms of aesthetics, so do explore.

(ref:pheatmap) Heatmap created using `pheatmap` with breaks to highlight clusters.

```{r pheatmap, fig.cap='(ref:pheatmap)',fig.asp=1, out.width= '80%', fig.align='center'}
dat_scaled %>% 
  # Rename the comlums
  magrittr::set_colnames(c("Ctl 1", "Ctl 2", "Ctl 3",
                                    "Trt 1", "Trt 2", "Trt 3")) %>% 
  # Plot heatmap
  pheatmap(.,
           fontsize = 7,
           cutree_rows = 2, # Create breaks in heatmap
           cutree_cols = 2) # Create breaks in heatmap
```

## Venn diagram

Another common plot used in proteomics is the Venn diagram. For these I use
the `VennDiagram` package [@R-VennDiagram].

For example if we wanted to compare the protein identifications found in the
control and treatment sets of our data we could compare the protein accessions found
in each condition. To do this we need to transform the data for example using
the following steps:

1. 

```{r transform-venn}
# Transform data for Venn diagram to create long table with three columns 
venn_dat <- dat %>%
  # Drop protein description
  select(-protein_description) %>% 
  # Gather columns according to experiment type to create exp_type 
  # and concentration variables. Don't use the protein accession.
               gather(key = exp_type, value = concentration, -protein_accession)

venn_dat
```

```{r plot-venn}
venn_cntl_1 <- venn_dat %>% 
  filter(exp_type == "control_1" & !is.na(concentration)) %>% 
  pull(prot_id)

venn_cntl_2 <- venn_dat %>% 
  filter(exp_type == "control_2" & !is.na(concentration)) %>% 
  pull(prot_id)

venn_cntl_3 <- venn_dat %>% 
  filter(exp_type == "control_3" & !is.na(concentration)) %>% 
  pull(prot_id)

venn_list <- list("Control 1" = venn_cntl_1,
                  "Control 2" = venn_cntl_2,
                  "Control 3" = venn_cntl_3)

futile.logger::flog.threshold(futile.logger::ERROR, name = "VennDiagramLogger")

prot_venn <- venn.diagram(venn_list,NULL, 
               #height = 1000,
               #width = 1000,
               col = "transparent",
               fill = c("cornflowerblue", "green", "yellow"),
               alpha = 0.50,
               cex = 0.8,
               fontfamily = "sans",
               fontface = "bold",
               cat.col = c("darkblue", "darkgreen", "orange"),
               cat.cex = 0.8,
               #cat.pos = 0,
               #cat.dist = 0.07,
               cat.fontfamily = "sans",
               #rotation.degree = 270,
               margin = 0.2,
               main = "Proteins identified in control experiments",
               main.fontfamily = "sans",
               print.mode = c("raw","percent"),
               main.pos = c(0.5,0.9)
  )

grid.arrange(gTree(children = prot_venn))
```



## Peptide sequence logos

Finally, creating sequence logos from peptides is another common task, especially
if you are doing immunopeptidomics and would like to explore the fequency of
amino acid types at each position in a set of peptide sequences. The
`ggseqlogo` package enables us to do this `ggplot` style [@R-ggseqlogo].

Here using sample data that comes with the ggseqlogo package and illusrated
in the [ggseqlogo tutorial](https://omarwagih.github.io/ggseqlogo/).

As with the venn diagram, we need a list to contain our groups of 
peptides as they

```{r ggseqlogo}
data(ggseqlogo_sample)

ggseqlogo(seqs_aa, method='p')
```

<!--chapter:end:05-data-transformation.Rmd-->

# Going further {#going-further}

Here are a few links and suggestions about what else you might like to do with
R.

## Exporting figures

Exporting figures is best done using the following structure:

```{r export-plots, eval=FALSE}
# Open up a blank plot file, pdf,jpeg etc.
<plot_function("file",...)>
# Write the plot to the file
<plot_object>
# Close the file
dev.off()
```

For example to export the volcano plot from Figure \@ref(fig:nice-vplot) to a pdf, we do:

```{r export-pdf, eval=FALSE}
# Open up a blank plot file, pdf,jpeg etc.
pdf("volcano_plot.pdf")

# Write the plot to the file
dat_tf %>%
  # Add a threhold for significant observations
  mutate(threshold = if_else(log_fc >= 2 & log_pval >= 1.3 |
                               log_fc <= -2 & log_pval >= 1.3,"A", "B")) %>%
  # Plot with points coloured according to the threshold
  ggplot(aes(log_fc,log_pval, colour = threshold)) +
  geom_point(alpha = 0.5) + # Alpha sets the transparency of the points
  # Add dotted lines to indicate the threshold, semi-transparent
  geom_hline(yintercept = 1.3, linetype = 2, alpha = 0.5) + 
  geom_vline(xintercept = 2, linetype = 2, alpha = 0.5) +
  geom_vline(xintercept = -2, linetype = 2, alpha = 0.5) +
  # Set the colour of the points
  scale_colour_manual(values = c("A"= "red", "B"= "black")) +
  xlab("log2 fold change") + ylab("-log10 p-value") + # Relabel the axes
  theme_minimal() + # Set the theme
  theme(legend.position="none") # Hide the legend

# Close the file
dev.off()
```

If I had saved the plot to an object called `vplot` I would call that object
instead of making the plot using `dat_tf` as shown here.

[Here](https://www.stat.berkeley.edu/classes/s133/saving.html) is a general
guide to the various formats you can export to.

Alternatively, if you are working in `ggplot` you can use the `ggsave` function
as described in [R4DS 28.7](http://r4ds.had.co.nz/graphics-for-communication.html#saving-your-plots).


## Exporting data

There is a [full manual for the import and export of data](https://cran.r-project.org/doc/manuals/r-release/R-data.html) in R.
However here are few pointers:

### Writing to a file

One of the most portables way to share data is by writing to a csv file. These
files can be opened in many programs. The tidyverse package contains two
functions for csv files, `write_csv` and for Excel `write_excel_csv`. The
latter form adds a bit of metadata that tells Excel about the file encoding.
See [R4DS writing to a file](http://r4ds.had.co.nz/data-import.html#writing-to-a-file).

For example to write a csv file of `dat_tf` to a file called
`04072018_transformed_data.csv` to our working directory for sharing
with a colleague using excel, the code is of the form 
`<function>(<r-object>,"filename")` like so:

```{r write-file, eval=FALSE}
write_excel_csv(dat_tf,"04072018_transformed_data.csv")
```

Note that the file name is a string and is in quotes.

### For R

If you are exporting data to use yourself in R, the custom `.rds` format is a 
good choice and preserves the R structure.

In the tidyverse `write_rds` follows the same structure as `write_csv`.

You can read back in using `read_rds`.

## Joining the R community

It's worth joining the [RStudio Community](https://community.rstudio.com/) and
following community members on Twitter such as [Jenny Bryan](https://twitter.com/JennyBryan), [Hadley Wickham](https://twitter.com/hadleywickham), [Yihui Xie](https://twitter.com/xieyihui), [Mara Averick](https://twitter.com/dataandme),
[David Robinson](https://twitter.com/drob)
 and [Julia Silge](https://twitter.com/juliasilge).
 
 If you can afford [DataCamp](https://www.datacamp.com) then this is my preferred
 learning platform.
 
 And if you can't, then [swirl](https://swirlstats.com/) is free.

## Communication: creating reports, presentations and websites

[R Markdown](https://rmarkdown.rstudio.com/lesson-1.html) [@R-rmarkdown] enables us to do
[literate programming](https://en.wikipedia.org/wiki/Literate_programming), saving time as we can create analysis, reports, dashboards or web apps at the same time
as writing code. R Markdown can use multiple programming languages. 
See also [R4DS R Markdown](http://r4ds.had.co.nz/r-markdown.html) and
[R4DS R Markdown formats](http://r4ds.had.co.nz/r-markdown-formats.html).

You can use [blogdown](https://bookdown.org/yihui/blogdown/) to build websites.
I created this guide to [buidling an academic website with blogdown](http://ab604.github.io/docs/website_bookdown/).

### Using bookdown to write a thesis dissertaion 

I used the bookdown package to create these materials [@R-bookdown] and you can
use it to write a thesis dissertaion, as detailed very nicely in this blog by
[Edd Berry](https://eddjberry.netlify.com/post/writing-your-thesis-with-bookdown/).

## Machine Learning

If you are interested in machine learning, then [TensorFlow](https://tensorflow.rstudio.com/) is a good place to start, for 
example Leon Eyrich Jessen's [Deep Learning for Cancer Immunotherapy](https://tensorflow.rstudio.com/blog/dl-for-cancer-immunotherapy.html) tutorial.

## Version control

<!--chapter:end:06-going-further.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

