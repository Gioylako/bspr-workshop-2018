[
["index.html", "Data Science Workshop Overview Requirements", " Data Science Workshop British Society for Proteomic Research Meeting 2018 Alistair Bailey May 31 2018 Overview These lessons cover: An introduction to R and RStudio An introduction to the tidyverse Importing and transforming proteomics data Visualisation of proteomics analysis Requirements Up to date version of R and Rstudio The following R packages: install.packages(c(&quot;tidyverse&quot;,&quot;janitor&quot;)) "],
["intro.html", "Chapter 1 Introduction 1.1 What are R and RStudio? 1.2 Why learn R, or any language ? 1.3 Finding your way around RStudio 1.4 Where am I? 1.5 R projects 1.6 Naming things 1.7 Seeking help", " Chapter 1 Introduction There are many resources for learning R on the web, but much of this material derives from a Data Carpentry lesson using ecological data that I have previously reworked, which in turn takes a lot from Hadley Wickham’s R for Data Science. Follow the links to access those materials. In terms of philosophy: The primary motivation for using tools such as R is to get more done, in less time and with less pain. And the overall aim is to understand and communicate findings from our data. Figure 1.1: Data project workflow. As shown in Figure 1.1 of typical data analysis workflow, to acheive this aim we need to learn tools that enable us to perform the fundamental tasks of tasks of importing, tidying and often transforming the data. Transformation means for example, selecting a subset of the data to work with, or calculating the mean of a set of observations. We’ll cover that in Chapter 3. But first… 1.1 What are R and RStudio? “There are only two kinds of languages: the ones people complain about and the ones nobody uses” Bjarne Stroustrup R is a programming language that follows the philosophy laid down by it’s predecessor S. The philosophy being that users begin in an interactive environment where they don’t consciously think of themselves as programming. It was created in 1993, and documented in (Ihaka and Gentleman 1996). Reasons R has become popular include that it is both open source and cross platform, and that it has broad functionality, from the analysis of data and creating powerful graphical visualisations and web apps. Like all languages though it has limitations, for example the syntax is initially confusing. Take for example the word environment… 1.1.1 Environments An environment is where we bring our data to work with it. Here we work in a R envrionment, using the R language as a set of tools. RStudio is an integrated development environment, or IDE for R programming. It is regularly updated, and upgrading enables access to the latest features. The latest version can be downloaded here: http://www.rstudio.com/download 1.2 Why learn R, or any language ? We can write R code without saving it, but it’s generally more useful to write and save our code as a script. Working with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes. Learning R (or any programming language) and working with scripts forces you to have deeper understanding of what you are doing, facilitates your learning and comprehension of the methods you use: Writing and publishing code is important for reproducible resarch R has many thousands of packages covering many disciplines. R can work with many types of data. They is a large R community for development and support. Using R gives you control over your figures and reports. 1.3 Finding your way around RStudio Let’s begin by learning about RStudio, the Integrated Development Environment (IDE). We will use R Studio IDE to write code, navigate the files found on our computer, inspect the variables we are going to create, and visualize the plots we will generate. R Studio can also be used for other things (e.g., version control, developing packages, writing Shiny apps) that we don’t have time to cover during this workshop. R Studio is divided into “Panes”, see Figure 1.2. When you first open it, there are three panes,the console where you type commands, your environment/history (top-right), and your files/plots/packages/help/viewer (bottom-right). The enivronment shows all the R objects you have created or are using, such as data you have imported. The output pane can be used to view any plots you have created. Not opened at first start up is the fourth default pane: the script editor pane, but this will open as soon as we create/edit a R script (or many other document types). The script editor is where will be typing much of the time. Figure 1.2: The Rstudio Integrated Development Environment (IDE). The placement of these panes and their content can be customized (see menu, R Studio -&gt; Tools -&gt; Global Options -&gt; Pane Layout). One of the advantages of using R Studio is that all the information you need to write code is available ina single window. Additionally, with many shortcuts, auto-completion, and highlighting for the major file types you use while developing in R, R Studio will make typing easier and less error-prone. Time for a philosphical diversion… 1.3.1 What is real? At the start, we might consider our environment “real” - that is to say the objects we’ve created/loaded and are using are “real”. But it’s much better in the long run to consider our scripts as “real” - our scripts are where we write down the code that creates our objects that we’ll be using in our environment. As a script is a document, it is reproducible Or to put it another way: we can easily recreate an environment from our scripts, but not so easily create a script from an enivronment. To support this notion of thinking in terms of our scripts as real, we recommend turning off the preservation of workspaces between sessions by setting the Tools &gt; Global Options menu in R studio as shown in Figure @(fig:turn-off): Figure 1.3: Don’t save your workspace! 1.4 Where am I? R studio tells you where you are in terms of directory address like so: (ref:working-dir) Figure 1.4: (ref:working-dir) If you are unfamiliar with how computers structure folders and files, then consider a tree with a root from which the trunk extends and branches divide. In the image above, the ~ symbol represents a contraction of the path from the root to the ‘home’ directory (in Windows this is ‘Documents’) and then the forward slashes are the branches. (Note: Windows uses backslashes, Unix type systems and R use forwardslashes). It is good practice to keep a set of related data, analyses, and text self-contained in a single folder, called the working directory. All of the scripts within this folder can then use relative paths to files that indicate where inside the project a file is located (as opposed to absolute paths, which point to where a file is on a specific computer). Working this way makes it a lot easier to move your project around on your computer and share it with others without worrying about whether or not the underlying scripts will still work. Figure 1.5: A typical directory structure 1.5 R projects RStudio also has a facility to keep all files associated with a particular analysis together called a project. Creating a project creates a working directory for you and also remembers its location (allowing you to quickly navigate to it) and optionally preserves custom settings and open files to make it easier to resume work after a break. (ref:r-projects) Figure 1.6: (ref:r-projects) Below, we will go through the steps for creating an “R Project”: Start R Studio (presentation of R Studio -below- should happen here) Under the File menu, click on New project, choose New directory, then Empty project Enter a name for this new folder (or “directory”, in computer science), and choose a convenient location for it. This will be your working directory for the rest of the day (e.g., ~/bspr-workshop) Click on “Create project” Under the Files tab on the right of the screen, click on New Folder and create a folder named data within your newly created working directory. (e.g., ~/bspr-workshopdata) Create a new R script (File &gt; New File &gt; R script) and save it in your working directory (e.g. bspr-workshop-script.R) 1.6 Naming things Jenny Bryan has three principles for naming things that are well worth remembering. When you names something, a file or an object, ideally it should be: Machine readable (no whitespace, punctuation, upper AND lowercase…) Human readable (makes sense in 6 months or 2 years time) Plays well with default ordering (numerical or date order) 1.7 Seeking help If you need help with a specific R function, let’s say barplot(), you can type: ?barplot If you can’t find what you are looking for, you can use the rdocumention.org website that searches through the help files across all packages available. A Google or internet search “R &lt;task&gt;” will often either send you to the appropriate package documentation or a helpful forum question that someone else already asked, such as Stack Overflow. 1.7.1 Asking for help The key to get help from someone is for them to grasp your problem rapidly. You should make it as easy as possible to pinpoint where the issue might be. Try to use the correct words to describe your problem. For instance, a package is not the same thing as a library. Most people will understand what you meant, but others have really strong feelings about the difference in meaning. The key point is that it can make things confusing for people trying to help you. Be as precise as possible when describing your problem. If possible, try to reduce what doesn’t work to a simple reproducible example. If you can reproduce the problem using a very small data.frame instead of your 50,000 rows and 10,000 columns one, provide the small one with the description of your problem. When appropriate, try to generalize what you are doing so even people who are not in your field can understand the question. For instance instead of using a subset of your real dataset, create a small (3 columns, 5 row) generic one. For more information on how to write a reproducible example see this article by Hadley Wickham. References "],
["tidyverse.html", "Chapter 2 Getting started in R and the tidyverse", " Chapter 2 Getting started in R and the tidyverse An introduction to the tidyverse. "],
["transform.html", "Chapter 3 Importing and transforming proteomics data 3.1 Importing flat files 3.2 Calculating fold change and enrichment 3.3 Fold change and log-fold change", " Chapter 3 Importing and transforming proteomics data 3.1 Importing flat files 3.2 Calculating fold change and enrichment 3.3 Fold change and log-fold change Fold changes are ratios, the ratio of say protein expression before and after treatment, where a value larger than 1 for a protein implies that protein expression was greater after the treatment. In life sciences, fold change is often reported as log-fold change. Why is that? There are at least two reasons which can be shown by plotting. One is that ratios are not symmetrical around 1, so it’s difficult to observe both changes in the forwards and backwards direcion i.e. proteins where expression went up and proteins where expression went down due to treatment. When we transform ratios on a log scale, the scale becomes symmetric around 0 and thus we can now observe the distribution of ratios in terms of positive, negative or no change. set.seed(10) x &lt;- 2^(rnorm(100)) y &lt;- 2^(rnorm(100)) ratios &lt;- tibble(value = x/y, label = &quot;ratios&quot;) logratios &lt;- tibble(value = log2(ratios$value), label = &quot;logratios&quot;) bind_rows(ratios,logratios) %&gt;% ggplot(aes(value)) + geom_histogram(binwidth = 2, colour = &quot;grey50&quot;, fill = &quot;white&quot;) + ggplot2::facet_wrap(~ label) + theme_minimal() A second reason is that transforming values onto a log scale changes where the numbers actually occur when plotted on that scale. If we consider the log scale to represent magnitudes, then we can more easily see changes of small and large magnitudes when we plot the data. For example, a fold change of 32 times can be either a ratio 1/32 or 32/1. 1/32 is much closer to 1 than 32/1, but transformed to a log scale we see that in terms of magnitude of difference it is the same as 32/1. x2 &lt;- 2^seq(1,5) y_vals &lt;- c(rev(1/x2),1,x2) names &lt;- c(paste0(&quot;1/&quot;,rev(x2)),1,x2) x_vals &lt;- seq(along=y_vals) dat &lt;- tibble(x_vals,y_vals,names) p1 &lt;- ggplot(dat,aes(x_vals,y_vals, label = names)) + geom_text() + geom_hline(yintercept = 1) + theme_minimal() + labs(x = NULL, y = NULL) + scale_x_continuous(breaks = NULL) p2 &lt;- ggplot(dat,aes(x_vals,y_vals, label = names)) + geom_text() + geom_hline(yintercept = 1) + scale_y_continuous(trans = &quot;log2&quot;) + theme_minimal() + labs(x = NULL, y = NULL) + scale_x_continuous(breaks = NULL) plot_grid(p1,p2) "],
["fold-change-and-t-test.html", "Chapter 4 Fold change and t-test 4.1 Heatmap transformation", " Chapter 4 Fold change and t-test Load the data, we need multiple replicates for each condition. To be tidy each protein is a set of observations (rows) of the variables, which are the values recorded for each replicate (columns). Tidy up and deal with missing values, either impute or exclude missing values and normalise. Let’s consider our proteomics data as a distribution of values, one value for each protein in our experiment that together form a distribution. If we have replicate experiments we’ll therefore have multiple distributions. A quantile represents a region of distribution, for example the 0.95 quantile is the value such that 95% of the data lies below it. To normalise two or more distributions with each other without recourse to a reference distribution we: Rank the value in each experiment (represented in the columns) from lowest to highest. In other words identify the quantiles. Sort each experiment (the columns) from lowest to highest value. Calculate the mean across the rows for the sorted values. Then substitute these mean values back according to rank for each experiment to restore the original order. This results in the highest ranking observation in each experiment becoming the mean of the highest observations across all experiments, the second ranking observation in each experiment becoming the mean of the second highest observations across all experiments. Dave Tang’s Blog : Quantile Normalisation in R Rafael Irizarry’s tweet Figure 4.1: Quantile Normalisation. # Tidy data up dat_tidy &lt;- dat_em %&gt;% # Remove missing values drop_na() %&gt;% # Select and rename columns select(protein_accession = protein_key_protein_accession, protein_description, con1 = protein_ngram_on_column_emilly_bowler_wt_rep1_001_merged_spectrum_ia_final_protein, con2 = protein_ngram_on_column_emilly_bowler_wt_rep2_001_merged_spectrum_ia_final_protein, con3 = protein_ngram_on_column_emilly_bowler_wt_rep3_001_merged_spectrum_ia_final_protein, trt1 = protein_ngram_on_column_emilly_bowler_kogsk_rep1_001_merged_spectrum_ia_final_protein, trt2 = protein_ngram_on_column_emilly_bowler_kogsk_rep2_merged_ia_final_protein, trt3 = protein_ngram_on_column_emilly_bowler_kogsk_rep3_001_merged_spectrum_ia_final_protein) # dat_tidy &lt;- dat_char %&gt;% # # Remove missing values # drop_na() %&gt;% # # Select and rename columns # select(protein_accession = protein_entry_protein_accession, # protein_description, # con1 = 9, # con2 = 11, # con3 = 13, # trt1 = 21, # trt2 = 23, # trt3 = 25) # dat_tidy &lt;- dat_le %&gt;% # # Remove missing values # drop_na() %&gt;% # # Select and rename columns # select(protein_accession = protein_accession, # protein_description, # con1 = 5, # con2 = 7, # con3 = 11, # trt1 = 3, # trt2 = 9, # trt3 = 13) # Plot data dat_tidy %&gt;% ggplot(aes(log2(con1))) + geom_density() + geom_density(aes(log2(con2), colour = &quot;blue&quot;)) + geom_density(aes(log2(con3), colour = &quot;red&quot;)) dat_tidy %&gt;% ggplot(aes(log2(trt1))) + geom_density() + geom_density(aes(log2(trt2), colour = &quot;blue&quot;)) + geom_density(aes(log2(trt3), colour = &quot;red&quot;)) # Normalise to maximum column value dat_norm_max &lt;- dat_tidy %&gt;% mutate(wt1 = con1/max(con1), con2 = con2/max(con3), con3 = con3/max(con3), trt1 = trt1 / max(trt1), trt2 = trt2 / max(trt2), trt3 = trt3 / max(trt3) ) # Normalise # dat_rank &lt;- dat_tidy %&gt;% select(-c(1:2,6:8)) %&gt;% # apply(., 2, rank, ties.method=&quot;average&quot;) %&gt;% as.data.frame() # # dat_sort &lt;- dat_tidy %&gt;% select(-c(1:2,6:8)) %&gt;% # apply(., 2, sort) %&gt;% as.data.frame() # # dat_mean &lt;- dat_sort %&gt;% apply(., 1, mean) # # index_mean &lt;- function(my_idx, my_mean){ # return(my_mean[my_idx]) # } # # dat_norm &lt;- dat_rank %&gt;% # apply(.,2,index_mean, my_mean = dat_mean) %&gt;% # as.data.frame() # Quantile normalisation : the aim is to give different distributions the # same statistical properties quantile_normalisation &lt;- function(df){ # df_rank &lt;- apply(df,2,rank,ties.method=&quot;average&quot;) df_sorted &lt;- data.frame(apply(df, 2, sort)) df_mean &lt;- apply(df_sorted, 1, mean) index_to_mean &lt;- function(my_index, my_mean){ return(my_mean[my_index]) } df_final &lt;- apply(df_rank, 2, index_to_mean, my_mean=df_mean) %&gt;% as.data.frame() #rownames(df_final) &lt;- rownames(df) return(df_final) } #dt_con &lt;- dat_tidy %&gt;% select(-c(1:2,6:8)) #dt_trt &lt;- dat_tidy %&gt;% select(-c(1:5)) #dt_con_norm &lt;- quantile_normalisation(dt_con) #dt_trt_norm &lt;- quantile_normalisation(dt_trt) dt &lt;- dat_tidy %&gt;% select(-c(1:2)) %&gt;% quantile_normalisation() dat_norm &lt;- bind_cols(dat_tidy[,1:2],dt) #dat_norm &lt;- bind_cols(dat_tidy[,1:2],dt_con_norm,dt_trt_norm) # Have a look at the median normalised data glimpse(dat_norm) ## Observations: 1,095 ## Variables: 8 ## $ protein_accession &lt;chr&gt; &quot;4562_DHE3_HUMAN&quot;, &quot;14948_RS2_HUMAN&quot;, &quot;143... ## $ protein_description &lt;chr&gt; &quot;Glutamate dehydrogenase 1_ mitochondrial ... ## $ con1 &lt;dbl&gt; 8.7383500, 24.6381167, 0.6251333, 9.166716... ## $ con2 &lt;dbl&gt; 11.8365000, 28.3875333, 0.7185167, 11.9014... ## $ con3 &lt;dbl&gt; 8.666500, 31.284083, 0.646800, 4.532583, 5... ## $ trt1 &lt;dbl&gt; 13.337283, 14.171300, 1.584183, 5.452217, ... ## $ trt2 &lt;dbl&gt; 0.8607500, 14.4218000, 1.0512833, 4.244433... ## $ trt3 &lt;dbl&gt; 9.343483, 13.670950, 2.139083, 4.735550, 6... dat_norm %&gt;% ggplot(aes(log2(con1))) + geom_density() + geom_density(aes(log2(con2), colour = &quot;blue&quot;)) + geom_density(aes(log2(con3), colour = &quot;red&quot;)) + geom_density(aes(log2(trt3), colour = &quot;green&quot;)) # Save the median normalised data write_excel_csv(dat_norm,&quot;data/example_nomralised_proteomics_data.csv&quot;) Use t.test to perform Welch Two Sample t-test on untransformed data. This outputs the p-values we need for each protein. # T-test function for multiple experiments expriments_ttest &lt;- function(dt,grp1,grp2){ # Subset control group and convert to numeric x &lt;- dt[grp1] %&gt;% unlist %&gt;% as.numeric() # Subset treatment group and convert to numeric y &lt;- dt[grp2] %&gt;% unlist %&gt;% as.numeric() # Perform t-test result &lt;- t.test(x, y) # Return p-values return(result$p.value) } # Apply t-test function to data # array = dat, 1 = rows, FUN = expriments_ttest, and arguements # For median normalised data p_vals &lt;- apply(dat_norm,1,expriments_ttest, grp1 = c(3:5), grp2 = c(6:8)) # For maximum normalised data p_vals_max &lt;- apply(dat_norm_max,1,expriments_ttest, grp1 = c(3:5), grp2 = c(6:8)) # Plot histograms hist(p_vals) hist(p_vals_max) Perform log transformation of the observations for each protein. # Select columns and log data dat_log &lt;- dat_norm %&gt;% select(-c(protein_accession,protein_description)) %&gt;% log2() dat_max_log &lt;- dat_norm_max %&gt;% select(-c(protein_accession,protein_description)) %&gt;% log2() Calculate the mean observation for each protein under each condition. con &lt;- apply(dat_log[,1:3],1,mean) trt &lt;- apply(dat_log[,4:6],1,mean) con_max &lt;- apply(dat_max_log[,1:3],1,mean) trt_max &lt;- apply(dat_max_log[,4:6],1,mean) The log fold change is then the difference between condition 1 and condition 2. Plot a histogram to look at the distribution. # Calculate fold change dat_fc &lt;- con - trt dat_max_fc &lt;- con_max - trt_max # Plot histograms hist(dat_fc) hist(dat_max_fc) Create a combined table of log fold change and p-values for all the proteins for plotting a volcano plot. dat &lt;- data.frame(prots= dat_norm$protein_accession, logfc = dat_fc, pval = -1*log10(p_vals)) dat_max &lt;- data.frame(prots= dat_norm_max$protein_accession, logfc = dat_max_fc, pval = -1*log10(p_vals_max)) dat_max %&gt;% ggplot(aes(logfc,pval)) + geom_point() dat %&gt;% mutate(threshold = if_else(logfc &gt;= 2 &amp; pval &gt;= 1.5 | logfc &lt;= -2 &amp; pval &gt;= 1.5,&quot;A&quot;, &quot;B&quot;)) %&gt;% ggplot(aes(logfc,pval, colour = threshold)) + geom_point() + scale_colour_manual(values = c(&quot;A&quot;= &quot;red&quot;, &quot;B&quot;= &quot;black&quot;)) + xlab(&quot;log2 fold change&quot;) + ylab(&quot;-log10 p-value&quot;) + theme_minimal() 4.1 Heatmap transformation Clustering the data dat_mut &lt;- dat_norm %&gt;% mutate(pval = dat$pval, logfc = dat$logfc) %&gt;% filter(pval &gt;= 2 &amp; (logfc &gt;=2 | logfc &lt;= 2)) %&gt;% select(-c(2,9:10)) dat_sel &lt;- as.matrix.data.frame(dat_mut[,2:7]) %&gt;% log2() row.names(dat_sel) &lt;- dat_mut$protein_accession dat.tn &lt;- scale(t(dat_sel)) %&gt;% t() #dat.tn &lt;- t(dat.n) #dat.tn &lt;- dat_sel #gplots::heatmap.2(dat.tn, scale = &#39;row&#39;,trace=&quot;none&quot;) library(pheatmap) pheatmap(dat.tn,cutree_rows = 2, cutree_cols = 2) cal_z_score &lt;- function(x){ (x - mean(x)) / sd(x) } data_subset_norm &lt;- t(apply(dat_mut[,2:7], 1, cal_z_score)) row.names(data_subset_norm) &lt;- dat_mut$protein_accession pheatmap(data_subset_norm) d1 &lt;- dat.tn %&gt;% t() %&gt;% dist(.,method = &quot;euclidean&quot;, diag = FALSE, upper = FALSE) d2 &lt;- dat.tn %&gt;% dist(.,method = &quot;euclidean&quot;, diag = FALSE, upper = FALSE) # Clustering distance between experiments using Ward linkage c1 &lt;- hclust(d1, method = &quot;ward.D2&quot;, members = NULL) # Clustering distance between proteins using Ward linkage c2 &lt;- hclust(d2, method = &quot;ward.D2&quot;, members = NULL) # Check clustering by plotting dendrograms par(mfrow=c(2,1),cex=0.5) # Make 2 rows, 1 col plot frame and shrink labels plot(c1); plot(c2) # Plot both cluster dendrograms 2. Plot the data # Set colours for heatmap, 25 increments my_palette &lt;- colorRampPalette(c(&quot;blue&quot;,&quot;white&quot;,&quot;red&quot;))(n = 25) # Plot heatmap with heatmap.2 par(cex.main=0.75) # Shrink title fonts on plot gplots::heatmap.2(dat.tn, # Tidy, normalised data Colv=as.dendrogram(c1), # Experiments clusters in cols Rowv=as.dendrogram(c2), # Protein clusters in rows density.info=&quot;histogram&quot;, # Plot histogram of data and colour key trace=&quot;none&quot;, # Turn of trace lines from heat map col = my_palette, # Use my colour scheme cexRow=0.5,cexCol=0.75) # Amend row and column label fonts "],
["visualising-proteomics-data.html", "Chapter 5 Visualising proteomics data 5.1 Creating a volcano plot 5.2 Creating a heatmap", " Chapter 5 Visualising proteomics data Based on empircal research, there are some general rules on visulisations that are worth bearing in mind: Plot 5.1 Creating a volcano plot VOlcano plot is a plot of the log fold change in the observation between two conditions on the x-axis, for example the protein expression between treatment and control conditions. And on the y-axis is the corresponding p-value for each observation. The p-value representing the likelihood that an observed change is due to the different conditions rather than arising from natural variation in the fold change that might be observed if we performed many replications of the experiment. The aim of a volcano plot is to enable the viewer to quickly see the effect (if any) of an experiment with two conditions on many species i.e. proteins in terms of both increased and decreased expression. Like all plots it has it’s good and bad points, namely it’s good that we can visualise a lot of complex information in one plot. However this is also it’s main weakness, it’s rather complicated to understand in one glance. However, volcano plots are widely used in the literature, so there may be an amount of social proof giving rise to their popularity as opposed to their utility. 5.2 Creating a heatmap A "],
["going-further.html", "Chapter 6 Going further 6.1 Getting help and joining the R community 6.2 Communication: creating reports, presentations and websites", " Chapter 6 Going further 6.1 Getting help and joining the R community 6.2 Communication: creating reports, presentations and websites "],
["references.html", "References", " References "]
]
