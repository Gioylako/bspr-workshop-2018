# Creating scripts and importing data {#import}

Our analysis is of an example data set of observations for 7702 proteins
from cells in three control experiments and three treatment experiments.
The observations are signal intensity measurements from the mass spectrometer.
These intensities relate the concentration of protein observed in each experiment 
and under each condition.

We consider raw data as the data as we receive it. This doesn't mean it hasn't be
processed in some way, it just means it hasn't been processed by us.
Generally speaking we don't change the raw data file, what we do is import it and
create an object in R which we then transform.

So let's understand how to import some data.

## Some definitions

 + **Importing** means getting data into our R environment by creating an object 
 that we can then manipulate. The raw data file remains unchanged.
 + **Inspecting** means looking at the dataset to understand what it contains.
 + **Tidying** refers to getting data into a consistent format that makes it easy 
 to use in later steps.

### Rectangular data and flat formats {#file-formats}

Two further things to note:
 
 1.  Here we are only considering **rectangular data**, the sort that comes
 in rows and columns such as in a spreadsheet. Lots of our data types exist,
 such as images, but can also be
 handled by R. As mentioned in \@ref(biocondutor) genomic data in particular has 
 led to a project called [Bioconductor](http://bioconductor.org/) for the 
 development of analysis tools primarily in R, many of which deal with 
 non-rectangular data, but this is beyond the scope here.
 
 2. **Flat formats** are files that only contain plain text, with each line
 representing a set of observations and the variables separated by delimiters
 such as tabs, commas or spaces. Therefore there aren't multiple tables such
 as we'd get in an Excel file, or meta-data such as the colour highlighting of
 a cell in an Excel file. The advantages of flat files is that they can be
 opened and used by many different computing languages or programs. 
 So unless there is a good reason not to use a flat format, and there are good 
 reasons, they are the best way to store data in many situations.
 
## Using scripts

Using the console is useful, but as we build up a workflow, that is to say,
writing code to:

+ load packages 
+ load data
+ explore the data 
+ and output some results

Then it's much more useful to contain this in a script: a document of our code.

Why? When we write and save our code in scripts, we can re-use it, share it or 
edit it. But **most importantly a script is a record**.

Cmd/Ctrl + Shift + N will open a new script file up and you should see something
like Figure \@ref(fig:script-pane) with the script editor pane open:

(ref:script-pane) Rstudio with the script editor pane open.

```{r script-pane, fig.cap = '(ref:script-pane)', fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/rstudio_screenshot_four_panes.PNG")
```

## Running code

We can run a highlighted portion of code in your script if you click the Run
button at the top of the scripts pane as shown in Figure \@ref(fig:run-script). 

(ref:run-script) Scripts can be run by clicking the Source button.

```{r run-script, fig.cap= '(ref:run-script)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/run_script.PNG")
```

You can run the entire script by clicking the Source button.

Or we can run chunks of code if we split our script into sections, see below.

## Creating a R script

We first need to create a script that will form the basis of our analysis.

Go to the file menu and select New Files > R script. This should open the script
editor pane.

Now let's save the script, by going to File > Save and we should find ourselves
prompted to save the script in our Project Directory.

Following the advice about [naming things](#names) we can create a new R script
called `01-bspr-workshop-july-2018`.

This name is machine readable (no spaces or special characters), human readable,
and works well with default ordering by beginning with `01`.

## Setting up our environment

At the head of our script it's common to put a title, the name of the author
and the date, and any other useful information. This is created as comments 
using the `#` at the start of each line.

It's then usual to follow this by code to load the packages we need into our
our R environment using the `library()` function and providing the name of the
package we wish to load. Packages are collections of R functions.

Often we break the code up into regions by adding dashes (or equals symbols) 
to the comment line. This enables us to run chunks of the script separately from
running the whole script when using our code.

Here is a typical head for a script:

```{r set-up, eval=FALSE}
# My workshop script
# 7th July 2018
# Alistair Bailey

# Load packages ----------------------------------------------------------------
library(plyr)
library(tidyverse)
library(gplots)
library(pheatmap)
library(gridExtra)
library(VennDiagram)
library(ggseqlogo)
```

### Bioconductor {#biocondutor}

As an aside there are many proteomics specific R packages, these are generally
found through [Bioconductor](https://www.bioconductor.org/) which is a project
that was initiated in 2001 to create tools for the analysis of high-throughput 
genomic data, but also includes other 'omics data tools [@gentleman2004,@huber2015].

Exploring Bioconductor is beyond our scope here, but well worth exploring for
manipulation and analysis of raw data formats such as mzxml files.

## Importing data

Assuming our data is in a [flat format](#file-formats), we can import it into our 
environment using the tidyverse `readr` package. 

If our data was an excel file, we can use the tidyverse `readxl` package to 
import the data, but it will remove any meta-data and each table in the excel
file will become a separate R object as per tidy data principles.

For the purposes of this workshop we have a `csv` (comma separated variable) file.

If you haven't done so already
<a href="https://raw.githubusercontent.com/ab604/ab604.github.io/master/docs/070718-proteomics-example-data.csv" download>Click here to download</a> the example data and save it to our project 
directory. Check the `Files` pane to see it's there.

We then import data and assign it to an object we'll call `data` like so:

```{r example-data, echo=FALSE, eval=F,cache=T}
# Charlotte's data
dat <- read_csv("data/MergedFinalProt_2colPerFile_12files_filtered_ch.csv",
                   na = "NaN", col_types = cols()) %>% clean_names()

# Clean it up for workshop, keep missing values
dat_select <- dat %>%
#   # Select and rename columns
  select(protein_accession = protein_entry_protein_accession,
         protein_description,
         control_1 = c1,
         control_2 = c2,
         control_3 = c3,
         treatment_1 = rt1,
         treatment_2 = rt2,
         treatment_3 = rt3)

# Write out the example data
write_csv(dat_select,"data/070718-proteomics-example-data.csv")
```

```{r import-data, eval=TRUE, cache=F}
# Import example data ----------------------------------------------------------
# Import the example data with read_csv from the readr package
dat <- readr::read_csv("data/070718-proteomics-example-data.csv")
```

## Exploring the data

### `glimpse`, `head` and `str`

The first thing to do with any data set is to actually look at it. Here are
four ways to have look at the data in the `Console`: calling the object directly, 
`glimpse`, `head` and `str`.

1. We can just call the object and return it to the `Console`, which may or may
not be useful depending on the size and type of object we call.

2 .`glimpse` is a tidyverse function that tries to show us as much data in
a data.frame or tibble as possible, telling us the [atomic types](#atomics) of 
data in the table, the number of observations and the number of variables, and 
importantly shows all the column variable names by transposing the table.

3. `head` is a base function that shows us the 6 lines of a R object by default.

4. `str` is a base function that show the structure of a R object, so it provides
a lot of information, but is not so easy to read.

The outputs for these four functions is shown below:

```{r examine-data}
# call object
dat

# tidyverse glimpse function
glimpse(dat)

# head function
head(dat)

# str function
str(dat)
```

To see the data in a *spreadsheet* fashion use `View(dat)`, note the capital V
and a new tab will open. This can also be launched from the `Environment` tab by
clicking on `dat`.

Although this provides us with some useful information, such as the number of
observations and variables, to understand more plotting the data will be
helpful as we'll see in Section \@ref(normalisation).

### Summary statisitics

Another useful way to quickly get a sense of the data is to use the `summary`
function, which will return summary of the spread of the data and importantly
if there are missing values. We can see immediately below that the experimental
replicates have different distributions, and missing values that we need to 
deal with in Chapter \@ref(transform).

```{r summary-stats}
summary(dat)
```
