# Transformation and visualisation {#transform}

Having imported our data set of observations for 7702 proteins
from cells in three control experiments and three treatment experiments.
Remember, the observations are signal intensity measurements from the mass spectrometer,
and these intensities relate to the amount of protein in each experiment and under 
each condition.

Next we will transform the data to examine the effect of
the treatment on the cellular proteome and visualise the output using a volcano
plot and a heatmap.

## Fold change and log-fold change

Fold changes are ratios, the ratio of say protein expression before and
after treatment, where a value larger than 1 for a protein implies that 
protein expression was greater after the treatment.

In life sciences, fold change is often reported as log-fold change. Why is that?
There are at least two reasons which can be shown by plotting.

One is that ratios are not symmetrical around 1, so it's difficult to observe
both changes in the forwards and backwards direcion i.e. proteins where expression
went up and proteins where expression went down due to treatment. When we 
transform ratios on a log scale, the scale becomes symmetric around 0 and thus
we can now observe the distribution of ratios in terms of positive, negative or
no change.

(ref:logratios) Ratios are not symmetric around one, logratios are symmetric around zero.

```{r fold-change-1,fig.cap='(ref:logratios)', echo=FALSE, cache=TRUE}
set.seed(10)
x <- 2^(rnorm(100))
y <- 2^(rnorm(100))
ratios <- tibble(value = x/y, label = "ratios")
logratios <- tibble(value = log2(ratios$value), label = "logratios")


bind_rows(ratios,logratios) %>% 
  mutate(label = factor(label, levels = c("ratios","logratios"))) %>% 
  ggplot(aes(value)) +
  geom_histogram(binwidth = 2, colour = "grey50", fill = "white") +
  ggplot2::facet_wrap(~ label) +
  xlab("") +
  ylab("") +
  theme_minimal()
```

A second reason is that transforming values onto a log scale changes where
the numbers actually occur when plotted on that scale. If we consider the log
scale to represent magnitudes, then we can more easily see changes of small and
large magnitudes when we plot the data.

For example, a fold change of 32 times can be either a ratio 1/32 or 32/1. 

As shown in Figure \@ref(fig:fold-change-2), 1/32 is much closer to 1 than 32/1, but 
transformed to a log scale we see that in terms of magnitude of difference it is the same as 32/1.

(ref:logratio-2) Transformation of scales using log transformation.

```{r fold-change-2, fig.cap='(ref:logratio-2)', echo=FALSE, cache=TRUE}
x2 <- 2^seq(1,5)
y_vals <- c(rev(1/x2),1,x2)
names <- c(paste0("1/",rev(x2)),1,x2)
x_vals <- seq(along=y_vals)

sim_dat <- tibble(x_vals,y_vals,names)

p1 <- ggplot(sim_dat,aes(x_vals,y_vals, label = names)) +
  geom_text() +
  geom_hline(yintercept = 1) +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = NULL)


p2 <- ggplot(sim_dat,aes(x_vals,y_vals, label = names)) +
  geom_text() +
  geom_hline(yintercept = 1) +
  scale_y_continuous(trans = "log2") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = NULL)

plot_grid(p1,p2)
```

## Dealing with missing values and normalisation {#normalisation}

<!---
1. Load the data, we need multiple replicates for each condition. To be tidy
each protein is a set of observations (rows) of the variables, which are the
values recorded for each replicate (columns).

2. Tidy up and deal with missing values, either impute or exclude missing values
and normalise. --->

Let's consider our proteomics data as a distribution of values, one value for 
each protein in our experiment that together form a distribution. 
If we have replicate experiments we'll therefore have multiple distributions.

A quantile represents a region of distribution, for example the 0.95 quantile
is the value such that 95% of the data lies below it. To normalise two or more
distributions with each other without recourse to a reference distribution we:

(i) Rank the value in each experiment (represented in the columns) from 
lowest to highest. In other words identify the quantiles.
(ii) Sort each experiment (the columns) from lowest to highest value.
(iii) Calculate the mean across the rows for the sorted values.
(iv) Then substitute these mean values back according to rank for each experiment to restore the original order.

This results in the highest ranking observation in each experiment
becoming the mean of the highest observations across all experiments, the
second ranking observation in each experiment becoming the mean of the 
second highest observations across all experiments.

[Dave Tang's Blog:Quantile Normalisation in R](https://davetang.org/muse/2014/07/07/quantile-normalisation-in-r/) has more
details on this approach.

(ref:quant-norm) Quantile Normalisation from [Rafael Irizarry's tweet](https://twitter.com/rafalab/status/545586012219772928?ref_src=twsrc%5Etfw).

```{r quant-norm, fig.cap='(ref:quant-norm)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/quant_norm.png")
```


```{r missing values, cache=TRUE}
# Remove the missing values
dat_tidy <- dat %>% drop_na()

glimpse(dat_tidy)
```

```{r data-distribution, cache=TRUE}
# Plot data
d1 <- dat_tidy %>%
  gather(experiment,value,-c(1:2)) %>%
  ggplot(aes(log2(value),colour = experiment)) +
   geom_density() 

d1
```

```{r max-normalisation, cache=TRUE}
# Normalise to maximum column value
dat_norm_max <- dat_tidy %>%
  mutate(control_1 = control_1/max(control_1),
         control_2 = control_2/max(control_2),
         control_3 = control_3/max(control_3),
         treatment_1 = treatment_1 / max(treatment_1),
         treatment_2 = treatment_2 / max(treatment_2),
         treatment_3 = treatment_3 / max(treatment_3)
         )

```

```{r quant-normalisation, cache=TRUE}
# Quantile normalisation : the aim is to give different distributions the
# same statistical properties
quantile_normalisation <- function(df){
  df_rank <- apply(df,2,rank,ties.method="average")
  df_sorted <- data.frame(apply(df, 2, sort))
  df_mean <- apply(df_sorted, 1, mean)
   
  index_to_mean <- function(my_index, my_mean){
    return(my_mean[my_index])
  }
   
  df_final <- apply(df_rank, 2, index_to_mean, my_mean=df_mean) %>% 
    as.tibble()
  
  return(df_final)
}
```

```{r qnorm-data, cache=TRUE}
dat_norm <- dat_tidy %>% select(-c(1:2)) %>% 
  quantile_normalisation() %>% 
  bind_cols(dat_tidy[,1:2],.)

glimpse(dat_norm)
```

```{r compare-normalisation, fig.asp=0.5, out.width= '80%', fig.align='center',cache=TRUE}
# Plot normalised data
d2 <- dat_norm %>%
  gather(key = experiment,value,-c(1:2)) %>% 
  ggplot(aes(log2(value),colour = experiment)) +
   geom_density()

plot_grid(d1,d2)
```

3. Use `t.test` to perform Welch Two Sample t-test on untransformed data. 
This outputs the p-values we need for each protein.

```{r t-test-function, cache=TRUE}
# T-test function for multiple experiments
expriments_ttest <- function(dt,grp1,grp2){
  # Subset control group and convert to numeric
  x <- dt[grp1] %>% unlist %>% as.numeric()
  # Subset treatment group and convert to numeric
  y <- dt[grp2] %>% unlist %>% as.numeric()
  # Perform t-test
  result <- t.test(x, y)
  # Return p-values
  return(result$p.value)
} 
```


```{r t-tests, cache=TRUE}
# Apply t-test function to data
# array = dat, 1 = rows, FUN = expriments_ttest, and arguements
# For median normalised data
p_vals <- apply(dat_norm,1,expriments_ttest, grp1 = c(3:5), grp2 = c(6:8))

p_vals_max <- apply(dat_norm_max,1,expriments_ttest, grp1 = c(3:5), grp2 = c(6:8))

# Plot histograms
hist(p_vals)
hist(p_vals_max)
```

4. Perform log transformation of the observations for each protein.

```{r log-data, cache=TRUE}
# Select columns and log data
dat_log <- dat_norm %>% 
  select(-c(protein_accession,protein_description)) %>% log2()
dat_max_log <- dat_norm_max %>%
  select(-c(protein_accession,protein_description)) %>% log2()
```

5. Calculate the mean observation for each protein under each condition.

```{r mean-log, cache=TRUE}
con <- apply(dat_log[,1:3],1,mean)
trt <- apply(dat_log[,4:6],1,mean)

con_max <- apply(dat_max_log[,1:3],1,mean)
trt_max <- apply(dat_max_log[,4:6],1,mean)
```

6. The log fold change is then the difference between condition 1 and condition 2.

```{r log-fc, cache=TRUE}
# Plot a histogram to look at the distribution.

# Calculate fold change
dat_fc <- con - trt
dat_max_fc <- con_max - trt_max

# Plot histograms
hist(dat_fc)
hist(dat_max_fc)
```

7. Create a combined table of log fold change and p-values for all the proteins
for plotting a volcano plot.

```{r volcano-plot, cache=TRUE}
dat <- tibble(prots= dat_norm$protein_accession,
                      logfc = dat_fc, 
                      pval = -1*log10(p_vals))

dat_max <- tibble(prots= dat_norm_max$protein_accession,
                      logfc = dat_max_fc,
                      pval = -1*log10(p_vals_max))

dat_max %>% ggplot(aes(logfc,pval)) + geom_point()

dat %>% 
  mutate(threshold = if_else(logfc >= 2 & pval >= 1.5 | 
                               logfc <= -2 & pval >= 1.5,"A", "B")) %>% 
  ggplot(aes(logfc,pval, colour = threshold)) + 
  geom_point() +
  scale_colour_manual(values = c("A"= "red", "B"= "black")) +
  xlab("log2 fold change") + ylab("-log10 p-value") +
  theme_minimal()
```


## Heatmap transformation

1. Clustering the data

```{r heatmap-2}
dat_mut <- dat_norm %>%
  mutate(pval = dat$pval, logfc = dat$logfc) %>%
  filter(pval >= 2 & (logfc >=2 | logfc <= 2)) %>%
  select(-c(2,9:10))

dat_sel <- as.matrix.data.frame(dat_mut[,2:7]) %>% log2()
row.names(dat_sel) <- dat_mut$protein_accession

dat.tn <- scale(t(dat_sel)) %>% t()
#dat.tn <- t(dat.n)
#dat.tn <- dat_sel

#gplots::heatmap.2(dat.tn, scale = 'row',trace="none")
library(pheatmap)
pheatmap(dat.tn,cutree_rows = 2,
         cutree_cols = 2)

cal_z_score <- function(x){
  (x - mean(x)) / sd(x)
}
 
data_subset_norm <- t(apply(dat_mut[,2:7], 1, cal_z_score))
row.names(data_subset_norm) <- dat_mut$protein_accession
pheatmap(data_subset_norm)

d1 <- dat.tn %>% t() %>% 
  dist(.,method = "euclidean", diag = FALSE, upper = FALSE)

d2 <- dat.tn %>% 
  dist(.,method = "euclidean", diag = FALSE, upper = FALSE)

# Clustering distance between experiments using Ward linkage
c1 <- hclust(d1, method = "ward.D2", members = NULL)
# Clustering distance between proteins using Ward linkage
c2 <- hclust(d2, method = "ward.D2", members = NULL)

# Check clustering by plotting dendrograms
par(mfrow=c(2,1),cex=0.5) # Make 2 rows, 1 col plot frame and shrink labels
plot(c1); plot(c2) # Plot both cluster dendrograms
```
2. Plot the data

```{r heatmap-1}
# Set colours for heatmap, 25 increments
my_palette <- colorRampPalette(c("blue","white","red"))(n = 25)

# Plot heatmap with heatmap.2
par(cex.main=0.75) # Shrink title fonts on plot
gplots::heatmap.2(dat.tn,                     # Tidy, normalised data
          Colv=as.dendrogram(c1),     # Experiments clusters in cols
          Rowv=as.dendrogram(c2),     # Protein clusters in rows
          density.info="histogram",   # Plot histogram of data and colour key
          trace="none",               # Turn of trace lines from heat map
          col = my_palette,           # Use my colour scheme
          cexRow=0.5,cexCol=0.75)     # Amend row and column label fonts
```

## Visualising proteomics data

Based on empircal research, there are some general rules on visulisations
that are worth bearing in mind:

1. Plot

## Creating a volcano plot

VOlcano plot is a plot of the log fold change in the observation between two
conditions on the x-axis, for example the protein expression between treatment 
and control conditions. And on the y-axis is the corresponding p-value for each
observation. The p-value representing the likelihood that an observed change
is due to the different conditions rather than arising from natural variation 
in the fold change that might be observed if we performed many replications of 
the experiment.

The aim of a volcano plot is to enable the viewer to quickly see the effect
(if any) of an experiment with two conditions on many species i.e. proteins
in terms of both increased and decreased expression.

Like all plots it has it's good and bad points, namely it's good that we can
visualise a lot of complex information in one plot. However this is also it's 
main weakness, it's rather complicated to understand in one glance.

However, volcano plots are widely used in the literature, so there may be an 
amount of [social proof](https://en.wikipedia.org/wiki/Social_proof) giving rise
to their popularity as opposed to their utility.

## Creating a heatmap

A 