# Transformation and visualisation {#transform}

Having imported our data set of observations for 7702 proteins
from cells in three control experiments and three treatment experiments.
Remember, the observations are signal intensity measurements from the mass 
spectrometer, and these intensities relate to the amount of protein in each 
experiment and under each condition.

Next we will transform the data to examine the effect of
the treatment on the cellular proteome and visualise the output using a volcano
plot and a heatmap. Concretely, the hypothesis we are testing is that treatment
changes the concentration of protein we observe.

## Fold change and log-fold change

Fold changes are ratios, the ratio of say protein expression before and
after treatment, where a value larger than 1 for a protein implies that 
protein expression was greater after the treatment.

In life sciences, fold change is often reported as log-fold change. Why is that?
There are at least two reasons which can be shown by plotting.

One is that ratios are not symmetrical around 1, so it's difficult to observe
both changes in the forwards and backwards direcion i.e. proteins where expression
went up and proteins where expression went down due to treatment. When we 
transform ratios on a log scale, the scale becomes symmetric around 0 and thus
we can now observe the distribution of ratios in terms of positive, negative or
no change.

(ref:logratios) Ratios are not symmetric around one, logratios are symmetric around zero.

```{r fold-change-1,fig.cap='(ref:logratios)', echo=FALSE, cache=TRUE}
set.seed(10)
x <- 2^(rnorm(100))
y <- 2^(rnorm(100))
ratios <- tibble(value = x/y, label = "ratios")
logratios <- tibble(value = log2(ratios$value), label = "logratios")


bind_rows(ratios,logratios) %>% 
  mutate(label = factor(label, levels = c("ratios","logratios"))) %>% 
  ggplot(aes(value)) +
  geom_histogram(binwidth = 2, colour = "grey50", fill = "white") +
  ggplot2::facet_wrap(~ label) +
  xlab("") +
  ylab("") +
  theme_minimal()
```

A second reason is that transforming values onto a log scale changes where
the numbers actually occur when plotted on that scale. If we consider the log
scale to represent magnitudes, then we can more easily see changes of small and
large magnitudes when we plot the data.

For example, a fold change of 32 times can be either a ratio 1/32 or 32/1. 

As shown in Figure \@ref(fig:fold-change-2), 1/32 is much closer to 1 than 32/1, 
but transformed to a log scale we see that in terms of magnitude of difference it 
is the same as 32/1.

Often the log transformation is to a base of 2 as each increment of 1 represents
a doubling, but sometimes a base of 10 is used, for example for p-values.

(ref:logratio-2) Transformation of scales using log transformation.

```{r fold-change-2, fig.cap='(ref:logratio-2)', echo=FALSE, cache=TRUE}
x2 <- 2^seq(1,5)
y_vals <- c(rev(1/x2),1,x2)
names <- c(paste0("1/",rev(x2)),1,x2)
x_vals <- seq(along=y_vals)

sim_dat <- tibble(x_vals,y_vals,names)

p1 <- ggplot(sim_dat,aes(x_vals,y_vals, label = names)) +
  geom_text() +
  geom_hline(yintercept = 1) +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = NULL)


p2 <- ggplot(sim_dat,aes(x_vals,y_vals, label = names)) +
  geom_text() +
  geom_hline(yintercept = 1) +
  scale_y_continuous(trans = "log2") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = NULL)

plot_grid(p1,p2)
```

## Dealing with missing values {#missing-values}

<!---
1. Load the data, we need multiple replicates for each condition. To be tidy
each protein is a set of observations (rows) of the variables, which are the
values recorded for each replicate (columns).

2. Tidy up and deal with missing values, either impute or exclude missing values
and normalise. --->

Unless we're really lucky, it's unlikely that we'll get observations for
the same numbers of proteins in all replicated experiments. This means
there will be missing values for some proteins when looking at all the 
experiments together. This then raises the question of what to do about the
missing values? We have two choices:

1. Only analyse the proteins that we have observations for in all experiments.
2. Impute values for the missing values from the existing observations.

There are pros and cons to either approach. Here for simplicity we'll use only
the proteins for which we have observations in all assays.

We can drop the proteins with missing values by piping our data set to the
`drop_na()` function from the `tidyr` package like so. We assign this to a new
object called `dat_tidy`.

```{r missing values, cache=F}
# Remove the missing values
dat_tidy <- dat %>% drop_na()

# Nunber of proteins in original data
dat %>% summarise(Number_of_proteins = n())
# Nunber of proteins without missing values
dat_tidy %>% summarise(Number_of_proteins = n())
```

This shrinks the dataset from 7,702 proteins to 1,145 proteins, so we can see 
why imputing the missing values might be more atrractive.

One approach you might like to try is to impute the data by replacing the 
missing values with the mean observation for each protein under each condition.

## Data normalization {#normalisation}

To perform statistical inference, for example whether treatment increases or
decreases protein abundance, we need to account for the variation that occurs
from run to run on our spectrometers and each give rise to a different 
distribution. This is as opposed to variation arising from treatment versus
control which we are interested in understanding. Hence normalization seeks to 
reduce the run-to-run sources of variation.

A method of normalization introduced for DNA microarray analysis is 
quantile normalization [@bolstad2003].

If we consider our proteomics data as a distribution of values, one value for 
the concentration of each protein in our experiment that together form a 
distribution. Figure \@ref(fig:data-dist) shows the distribution of
protein concentrations observed for the three control and three treatment assays.
As we can see the distributions are different for each assay.

(ref:prot-dist) Protein data for six assays plotted as a distributions.

```{r data-dist, fig.cap='(ref:prot-dist)',fig.asp=0.5, out.width= '80%', fig.align='center', echo=FALSE, cache=TRUE}
# Plot data
d1 <- dat_tidy %>%
  gather(experiment,value,-c(1:2)) %>%
  ggplot(aes(log2(value),colour = experiment)) +
  geom_density() +
  xlab("") +
  theme_minimal()

d1
```

A quantile represents a region of distribution, for example the 0.95 quantile
is the value such that 95% of the data lies below it. To normalize two or more
distributions with each other without recourse to a reference distribution we:

(i) Rank the value in each experiment (represented in the columns) from 
lowest to highest. In other words identify the quantiles for each protein 
in each experiment.
(ii) Sort each experiment (the columns) from lowest to highest value.
(iii) Calculate the mean across the rows for the sorted values.
(iv) Then substitute these mean values back according to rank for each experiment 
to restore the original order.

This results in the highest ranking observation in each experiment
becoming the mean of the highest observations across all experiments, the
second ranking observation in each experiment becoming the mean of the 
second highest observations across all experiments. Therefore the 
distributions for each each experiment are now the same.

[Dave Tang's Blog:Quantile Normalisation in R](https://davetang.org/muse/2014/07/07/quantile-normalisation-in-r/) has more
details on this approach.

(ref:quant-norm) Quantile Normalisation from [Rafael Irizarry's tweet](https://twitter.com/rafalab/status/545586012219772928?ref_src=twsrc%5Etfw).

```{r quant-norm, fig.cap='(ref:quant-norm)',fig.asp=1, out.width= '80%', fig.align='center', echo=FALSE}
knitr::include_graphics("img/quant_norm.png")
```

These result of quantile normalization is that our distributions become
statisitcally identitical, which we can see by plotting the densities of the
normalized data. As shown in Figure \@ref(fig:compare-normalisation) the distributions
all overlay.

```{r quant-normalisation, cache=TRUE}
# Quantile normalisation : the aim is to give different distributions the
# same statistical properties
quantile_normalisation <- function(df){
  
  # Find rank of values in each column
  df_rank <- map_df(df,rank,ties.method="average")
  # Sort observations in each column from lowest to highest 
  df_sorted <- map_df(df,sort)
  # Find row mean on sorted columns
  df_mean <- rowMeans(df_sorted)
  
  # Function for substiting mean values according to rank 
  index_to_mean <- function(my_index, my_mean){
    return(my_mean[my_index])
  }
  
  # Replace value in each column with mean according to rank 
  df_final <- map_df(df_rank,index_to_mean, my_mean=df_mean)
  
  return(df_final)
}
```

```{r qnorm-data, cache=F}
dat_norm <- dat_tidy %>% select(-c(1:2)) %>% 
  quantile_normalisation() %>% 
  bind_cols(dat_tidy[,1:2],.)
```

(ref:compare-qnorm) Comparison of the protein distributions before normalization (left)
and after quantile normalization (right).

```{r compare-normalisation, fig.cap='(ref:compare-qnorm)', out.width= '80%', fig.asp= 0.5, fig.align='center', echo = FALSE, cache=F}
# Plot normalised data
d2 <- dat_norm %>%
  gather(key = experiment,value,-c(1:2)) %>% 
  ggplot(aes(log2(value),colour = experiment)) +
  geom_density() +
  xlab("") +
  theme_minimal() +
  theme(legend.position="none")
  

d1c <- d1 + theme(legend.position="none")

plot_grid(d1c,d2)
# dat_norm %>% select(-protein_accession,-protein_description) %>% 
#   ggplot(aes(sample = log2(control_1))) + geom_qq()
```

### T-test

Having removed missing values and normalised the data, we can consider our
hypothesis: treatement changes the amount of protein we observe in the cells.

Things we need to consider in performing our t-test:

1. We are perfoming a test for each protein for between three control and 
treatment experiments and assume unequal variances between the control and 
treatment for each protein. A look at a single protein supports this assumption:

```{r uneqal-variances}
# # Variance of first control protein observations
# var(c(dat_norm$control_1[1], dat_norm$control_2[1], dat_norm$control_3[1]))
# # Variance of first treatment protein observations
# var(c(dat_norm$treatment_1[1], dat_norm$treatment_2[1], dat_norm$treatment_3[1]))
# 
# treatment <- c(dat_norm$treatment_1[1], 
#                dat_norm$treatment_2[1], 
#                dat_norm$treatment_3[1])
# control <- c(dat_norm$control_1[1],
#              dat_norm$control_2[1],
#              dat_norm$control_3[1])
# 
# t.test(treatment,control)$p.value
```

Hence we will perform a Welch's t-test for unequal variances.

2. We don't know whether the effect of the treatment is to increase or decrease
the concentration of the protein, hence we will perform a two-sided t-test.

3. The observations for the proteins are for proteins of the same type but from 
independent experiments, rather than observations of the same individuals
before and after treatment. Hence we test the observations as unpaired samples.

Use `t.test` to perform Welch Two Sample t-test on untransformed data. 
This outputs the p-values we need for each protein. I couldn't figure out a tidy
way to do this, so I am using the base R apply function here.

What's going on in the code below is the following,

```{r t-test-function, cache=TRUE}
# T-test function for multiple experiments
t_test <- function(dt,grp1,grp2){
  # Subset control group and convert to numeric
  x <- dt[grp1] %>% unlist %>% as.numeric()
  # Subset treatment group and convert to numeric
  y <- dt[grp2] %>% unlist %>% as.numeric()
  # Perform t-test using the mean of x and y
  result <- t.test(x, y)
  # Extract p-values from the results
  p_vals <- tibble(p_val = result$p.value)
  # Return p-values
  return(p_vals)
} 
```

```{r t-tests, cache=F}
# Apply t-test function to data using plyr adply
#  .margins = 1, slice by rows, .fun = t_test plus t_test arguements
dat_pvals <- plyr::adply(dat_norm,.margins = 1, .fun = t_test, 
                grp1 = c(3:5), grp2 = c(6:8))
```

```{r pval-table,echo=FALSE}
# Calculate the p-value for the first protein and compare with function
p_check <- tibble(p1= round(dat_pvals$p_val[1],4),
                  p2 =round(t.test(as.numeric(dat_norm[1,3:5]),
                    as.numeric(dat_norm[1,6:8]))$p.value,4))

knitr::kable(p_check %>% select("Function p-val" = p1, "p-val" = p2),
  booktabs=TRUE)
```

```{r p-val-plot}
# Plot histogram
dat_pvals %>% 
  ggplot(aes(p_val)) + 
  geom_histogram(binwidth = 0.05, 
           boundary = 0.5, 
           fill = "darkblue",
           colour = "white") +
  xlab("p-value") +
  ylab("Frequency") +
  theme_minimal()
```

4. Perform log transformation of the observations for each protein.

```{r log-data, cache=F}
# Select columns and log data
exp_log <- dat_pvals %>% 
  select(-c(protein_accession,protein_description,p_val)) %>% 
  log2() %>% as.tibble()

# Bind columns to create transformed data frame
dat_log <- bind_cols(dat_pvals[,c(1,2,9)], exp_log) %>%
  select(protein_accession,
                   protein_description,
                   control_1, control_2,control_3,
                   treatment_1,treatment_2,treatment_3,
                   p_val)
```

5. Calculate the mean observation for each protein under each condition.

```{r mean-log, cache=F}
# Group mean function for multiple experiments
grp_mean <- function(dt,grp1,grp2){
  # Subset control group and convert to numeric
  x <- dt[grp1] %>% unlist %>% as.numeric()
  # Subset treatment group and convert to numeric
  y <- dt[grp2] %>% unlist %>% as.numeric()
  # Calculate mean for each group
  mean_x <- mean(x)
  mean_y <- mean(y)
  # Make data frame of means
  mean_xy <- tibble(mean_grp1 = mean_x, mean_grp2 = mean_y)
  # Return p-values
  return(mean_xy)
} 

dat_fc <- plyr::adply(dat_log,.margins = 1, 
                        .fun = grp_mean, 
                grp1 = c(3:5), 
                grp2 = c(6:8)) %>% 
  # Calculate fold change as difference between mean control and mean treatment
  mutate(log_fc = mean_grp1 - mean_grp2) 

# Final transformed data
dat_tf <- dat_fc %>% select(protein_accession,
                            protein_description,
                            log_fc, p_val) %>% 
  mutate(log_pval = -1*log10(p_val)) %>% 
  select(-p_val)

knitr::kable(dat_tf %>% head(.,5),
  booktabs=TRUE)
```

6. The log fold change is then the difference between condition 1 and condition 2.

```{r log-fc, cache=F}
# Plot a histogram to look at the distribution.
dat_tf %>%
  ggplot(aes(log_fc)) + 
  geom_histogram(binwidth = 0.5,
                 boundary = 0.5,
           fill = "darkblue",
           colour = "white") +
  xlab("log2 fold change") +
  ylab("Frequency") +
  theme_minimal()
```

## Visualising data

Based on empircal research, there are some general rules on visulisations
that are worth bearing in mind:

1. Plot

## Creating a volcano plot

<!--- 7. Create a combined table of log fold change and p-values for all the proteins
for plotting a volcano plot. --->

A volcano plot is a plot of the log fold change in the observation between two
conditions on the x-axis, for example the protein expression between treatment 
and control conditions. On the y-axis is the corresponding p-value for each
observation, representing the likelihood that an observed change
is due to the different conditions rather than arising from a natural variation 
in the fold change that might be observed if we performed many replications of 
the experiment.

The aim of a volcano plot is to enable the viewer to quickly see the effect
(if any) of an experiment with two conditions on many species (i.e. proteins)
in terms of both an increase and decrease of the observed value.

Like all plots it has it's good and bad points, namely it's good that we can
visualise a lot of complex information in one plot. However this is also it's 
main weakness, it's rather complicated to understand in one glance.

However, volcano plots are widely used in the literature, so there may be an 
amount of [social proof](https://en.wikipedia.org/wiki/Social_proof) giving rise
to their popularity as much as their utility.


```{r volcano-plot, cache=F}
dat_tf %>% ggplot(aes(log_fc,log_pval)) + geom_point()
```

However it would be much more useful with some extra formatting...

```{r nice-vplot}
dat_tf %>%
  # Add a threhold for significant observations
  mutate(threshold = if_else(log_fc >= 2 & log_pval >= 1.5 |
                               log_fc <= -2 & log_pval >= 1.5,"A", "B")) %>%
  # Plot with points coloured according to the threshold
  ggplot(aes(log_fc,log_pval, colour = threshold)) +
  geom_point(alpha = 0.5) + # Alpha sets the transparency of the points
  # Add dotted lines to indicate the threshold, semi-transparent
  geom_hline(yintercept = 1.5, linetype = 2, alpha = 0.5) + 
  geom_vline(xintercept = 2, linetype = 2, alpha = 0.5) +
  geom_vline(xintercept = -2, linetype = 2, alpha = 0.5) +
  # Set the colour of the points
  scale_colour_manual(values = c("A"= "red", "B"= "black")) +
  xlab("log2 fold change") + ylab("-log10 p-value") + # Relabel the axes
  theme_minimal() + # Set the theme
  theme(legend.position="none") # Hide the legend
```

But which proteins are the significant observations?

```{r sig-obs}
dat_tf %>%
  # Add a threhold for significant observations
  mutate(threshold = if_else(log_fc >= 2 & log_pval >= 1.5 |
                               log_fc <= -2 & log_pval >= 1.5,"A", "B"),
         prot_id = str_extract(protein_accession,".{6}$")) %>%  # Get last six characters
  # Filter observations above the threshold
  filter(threshold == "A")
```

## Creating a heatmap plot

<!-- 1. Clustering the data -->

```{r heatmap-2}
# Keep the same p-val cut-off, but relax the log_fc to 1 which represents a 
# doubling
dat_mut <- dat_norm %>%
  mutate(log_pval = dat_tf$log_pval, log_fc = dat_tf$log_fc) %>%
  filter(log_pval >= 1.5 & (log_fc >= 1 | log_fc <= -1)) %>%
  select(-c(2,9:10))

dat_sel <- as.matrix.data.frame(dat_mut[,2:7]) %>% log2()
row.names(dat_sel) <- dat_mut$protein_accession

dat.tn <- scale(t(dat_sel)) %>% t()
#dat.tn <- t(dat.n)
#dat.tn <- dat_sel

#gplots::heatmap.2(dat.tn, scale = 'row',trace="none")

pheatmap(dat.tn,cutree_rows = 2,
         cutree_cols = 2,
         fontsize_row = 6)

cal_z_score <- function(x){
  (x - mean(x)) / sd(x)
}

data_subset_norm <- t(apply(dat_mut[,2:7], 1, cal_z_score))

data_subset_norm %>% 
  magrittr::set_colnames(c("Ctl 1", "Ctl 2", "Ctl 3",
                                    "Trt 1", "Trt 2", "Trt 3")) %>% 
  magrittr::set_rownames(dat_mut$protein_accession) %>% 
  pheatmap(.,
           fontsize = 6,
           cutree_rows = 2,
           cutree_cols = 2)

d1 <- dat.tn %>% t() %>%
  dist(.,method = "euclidean", diag = FALSE, upper = FALSE)

d2 <- dat.tn %>%
  dist(.,method = "euclidean", diag = FALSE, upper = FALSE)

# Clustering distance between experiments using Ward linkage
c1 <- hclust(d1, method = "ward.D2", members = NULL)
# Clustering distance between proteins using Ward linkage
c2 <- hclust(d2, method = "ward.D2", members = NULL)

# Check clustering by plotting dendrograms
par(mfrow=c(2,1),cex=0.5) # Make 2 rows, 1 col plot frame and shrink labels
plot(c1); plot(c2) # Plot both cluster dendrograms
```

<!-- 2. Plot the data -->

```{r heatmap-1}
# Set colours for heatmap, 25 increments
my_palette <- colorRampPalette(c("blue","white","red"))(n = 25)

# Plot heatmap with heatmap.2
par(cex.main=0.75) # Shrink title fonts on plot
dat.tn %>% 
  magrittr::set_colnames(c("Ctl 1", "Ctl 2", "Ctl 3",
                                    "Trt 1", "Trt 2", "Trt 3")) %>% 
  magrittr::set_rownames(dat_mut$protein_accession) %>% 
  gplots::heatmap.2(.,                     # Tidy, normalised data
          Colv=as.dendrogram(c1),     # Experiments clusters in cols
          Rowv=as.dendrogram(c2),     # Protein clusters in rows
          density.info="histogram",   # Plot histogram of data and colour key
          trace="none",               # Turn of trace lines from heat map
          col = my_palette,           # Use my colour scheme
          cexRow=0.3,cexCol=0.75)     # Amend row and column label fonts
```

